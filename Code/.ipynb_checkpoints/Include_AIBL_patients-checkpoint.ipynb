{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to merge file AIBL_data.csv with Merged_data.csv (which is the existing ADNI file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "#                                              Import libraries\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "#%reset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "import matplotlib as mpl\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sys import stdout\n",
    "import collections\n",
    "import csv\n",
    "import pandas as pd \n",
    "from scipy.stats import mode\n",
    "from scipy import stats\n",
    "from dateutil.parser import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_numerical_table(fileName):\n",
    "    \"\"\"Function to load .csv file that has numerical-only data; it also displays some basic info about the .csv file content\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    -fileName: path of rthe .csv file we want to load; put file name only if file is located in current path\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    InputMatrix  = numpy array with all values, equivalent to the values in the input .csv file (but no headers)\n",
    "    InputHeaders = list with headers in input .csv file\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Load .csv file into panda dataframe and show head\n",
    "    # ----------------------------------------------------\n",
    "    transformed_data = pd.read_csv(fileName)\n",
    "    display(transformed_data.head(3))\n",
    "\n",
    "    # Put headers into list\n",
    "    # ----------------------\n",
    "    InputHeaders=list(transformed_data)\n",
    "\n",
    "\n",
    "    # Turn data frames into matrix\n",
    "    # ---------------------------------\n",
    "    InputMatrix  = transformed_data.as_matrix(); InputMatrix = np.array(InputMatrix) ;\n",
    "    print \"Input Matrix Size:\"; print \"-----------------------\"\n",
    "    print InputMatrix.shape\n",
    "\n",
    "    # Turn -1000 into NaNs\n",
    "    # ---------------------------------\n",
    "    #InputMatrix[InputMatrix==-1000] = np.nan\n",
    "    \n",
    "    return InputMatrix,InputHeaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Data size after stacking ADNI and AIBL data sets:\n",
      "------------------------------------------------- \n",
      "(26959, 70)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Load ADNI and AIBL data sets\n",
    "# -----------------------------------------\n",
    "#\n",
    "ADNI_Data=pd.read_csv(\"Merged_data_original.csv\")\n",
    "AIBL_Data=pd.read_csv(\"AIBL_data.csv\")\n",
    "\n",
    "#Change the types of AIBL to match ADNI\n",
    "for i in list(ADNI_Data):\n",
    "    AIBL_Data[i] = AIBL_Data[i].astype(ADNI_Data[i].dtype)\n",
    "\n",
    "# Add 100000 to AIBL's RIDs to make sure they do not overlap with ADNI's RIDs\n",
    "AIBL_Data['RID']=AIBL_Data['RID']+100000\n",
    "\n",
    "# The timestamp columns given in the AIBL file do not have the right format; we overwrite them for now (it does not matter too\n",
    "# much as they are not really used in our analysis)\n",
    "AIBL_Data['update_stamp'] = [ADNI_Data['update_stamp'].loc[0]]*len(AIBL_Data)\n",
    "\n",
    "# Stack ADNI and AIBL data sets into one big matrix\n",
    "ADNI_AIBL_data = ADNI_Data.append(AIBL_Data)\n",
    "\n",
    "# Print matrix size after stacking ADNI and AIBL data sets\n",
    "print \" \"; print \"Data size after stacking ADNI and AIBL data sets:\";print \"------------------------------------------------- \";\n",
    "print ADNI_AIBL_data.shape\n",
    "\n",
    "# Save to .csv file\n",
    "ADNI_AIBL_data.to_csv('Merged_data.csv',header=list(AIBL_Data), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/courtneycochrane/anaconda/envs/py27/lib/python2.7/site-packages/nbformat/current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.worksheets[0].cells:\n",
    "                if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from LongitudinalDataAnalysis.ipynb\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Imputation.ipynb\n",
      "importing Jupyter notebook from FeatureReduction.ipynb\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from SupervisedLearning.ipynb\n",
      "importing Jupyter notebook from ModelPerformance.ipynb\n",
      "importing Jupyter notebook from TrainTestSplit.ipynb\n",
      "importing Jupyter notebook from Categorical_Updated.ipynb\n"
     ]
    }
   ],
   "source": [
    "import LongitudinalDataAnalysis\n",
    "import Imputation\n",
    "import FeatureReduction\n",
    "import SupervisedLearning\n",
    "import ModelPerformance\n",
    "import TrainTestSplit\n",
    "import Categorical_Updated\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that are one-hot encoded\n",
      "-------------------------------------\n",
      "['VISCODE', 'COLPROT', 'ORIGPROT', 'DX_bl', 'PTGENDER', 'PTETHCAT', 'PTRACCAT', 'PTMARRY']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Categorical to Numerical\n",
    "# -------------------------------\n",
    "date_cols = ['update_stamp','EXAMDATE','EXAMDATE_bl']\n",
    "cols_to_ignore = ['PTID']\n",
    "\n",
    "Categorical_Updated.categorical_conversion(date_cols,cols_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>RID</th>\n",
       "      <th>SITE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS11</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>...</th>\n",
       "      <th>PTRACCAT_Black</th>\n",
       "      <th>PTRACCAT_Hawaiian/Other PI</th>\n",
       "      <th>PTRACCAT_More than one</th>\n",
       "      <th>PTRACCAT_Unknown</th>\n",
       "      <th>PTRACCAT_White</th>\n",
       "      <th>PTMARRY_Married</th>\n",
       "      <th>PTMARRY_Never married</th>\n",
       "      <th>PTMARRY_Unknown</th>\n",
       "      <th>PTMARRY_Widowed</th>\n",
       "      <th>AD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>74.3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>18.67</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>81.3</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>81.3</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  RID  SITE   AGE  PTEDUCAT  APOE4  CDRSB  ADAS11  ADAS13  MMSE  \\\n",
       "0           0    2    11  74.3        16    0.0    0.0   10.67   18.67  28.0   \n",
       "1           1    3    11  81.3        18    1.0    4.5   22.00   31.00  20.0   \n",
       "2           2    3    11  81.3        18    1.0    6.0   19.00   30.00  24.0   \n",
       "\n",
       "  ...  PTRACCAT_Black  PTRACCAT_Hawaiian/Other PI  PTRACCAT_More than one  \\\n",
       "0 ...             0.0                         0.0                     0.0   \n",
       "1 ...             0.0                         0.0                     0.0   \n",
       "2 ...             0.0                         0.0                     0.0   \n",
       "\n",
       "   PTRACCAT_Unknown  PTRACCAT_White  PTMARRY_Married  PTMARRY_Never married  \\\n",
       "0               0.0             1.0              1.0                    0.0   \n",
       "1               0.0             1.0              1.0                    0.0   \n",
       "2               0.0             1.0              1.0                    0.0   \n",
       "\n",
       "   PTMARRY_Unknown  PTMARRY_Widowed  AD  \n",
       "0              0.0              0.0   0  \n",
       "1              0.0              0.0   1  \n",
       "2              0.0              0.0   1  \n",
       "\n",
       "[3 rows x 106 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix Size:\n",
      "-----------------------\n",
      "(26959, 106)\n",
      " \n",
      "Identified columns of interest in input file: \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Patient RID column: [1]\n",
      "Demo columns: [  3   4  92  93  94  95  96  97  98  99 100 101 102 103 104]\n",
      "BaselineOneTime columns: [ 5 54 85 86 87]\n",
      "BaselineEvaluation columns: [30 31 32 33 35 36 37 34 38 39 41 42 43 44 45 40 46 48 49 50 51 52 47 53]\n",
      "Time columns: [ 2 56 57 58 59 60 82 83 84 55]\n",
      "CurrentEvaluation columns: [ 6  7  8  9 11 12 13 10 14 15 17 18 19 20 21 16 22 24 25 26 27 28 23 29]\n",
      "CurrentDiagnosis columns: [105]\n",
      "------\n",
      "Method 2 for Longitudinal Data Analysis\n",
      "------\n",
      " \n",
      "New Input Matrix Size:\n",
      "-----------------------\n",
      "(4335, 116)\n",
      " \n",
      "New Output Matrix Size:\n",
      "-----------------------\n",
      "(4335,)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Longitudinal Data Analysis\n",
    "# -------------------------------\n",
    "\n",
    "# Input file name for Longitudinal Data Analysis\n",
    "InputToLongitudinal='CategoricalToNumerical.csv'\n",
    "\n",
    "with open(InputToLongitudinal) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    labels = next(reader)\n",
    "\n",
    "# Output file name from this script\n",
    "OutputFromLongitudinal='LongitudinalDataAnalysis.csv'\n",
    "\n",
    "# Patient RID Features\n",
    "Patient_FEATURES=['RID'];\n",
    "\n",
    "# Demographic Features\n",
    "Demo_FEATURES_type=['AGE','PTEDUCAT','PTGENDER','PTETHCAT','PTRACCAT','PTMARRY']\n",
    "Demo_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    \n",
    "# Baseline OneTime Features\n",
    "BaselineOneTime_FEATURES_type = ['APOE4','Years_bl','ORIGPROT']\n",
    "BaselineOneTime_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "    elif i.rfind(\"_\") != -1 and i[:i.rfind(\"_\")] in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "        \n",
    "# Time Headers\n",
    "Time_FEATURES_type=['SITE','Month','update_stamp_minus_EXAMDATE_bl','update_stamp_minus_EXAMDATE','EXAMDATE_minus_EXAMDATE_bl',\n",
    "               'COLPROT','M','Month_bl']\n",
    "Time_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "Time_FEATURES.insert(len(Time_FEATURES), Time_FEATURES.pop(Time_FEATURES.index('Month_bl'))) # Month_bl must be last feature in this list\n",
    "\n",
    "        \n",
    "# Baseline Evaluation Features\n",
    "BaselineEvaluation_FEATURES=['CDRSB_bl','ADAS11_bl','ADAS13_bl','MMSE_bl','RAVLT_learning_bl','RAVLT_forgetting_bl',\n",
    "                             'RAVLT_perc_forgetting_bl','RAVLT_immediate_bl','FAQ_bl','MOCA_bl','EcogPtLang_bl','EcogPtVisspat_bl',\n",
    "                             'EcogPtPlan_bl','EcogPtOrgan_bl','EcogPtDivatt_bl','EcogPtMem_bl','EcogPtTotal_bl','EcogSPLang_bl',\n",
    "                             'EcogSPVisspat_bl','EcogSPPlan_bl','EcogSPOrgan_bl','EcogSPDivatt_bl','EcogSPMem_bl','EcogSPTotal_bl'];\n",
    "\n",
    "\n",
    "   \n",
    "# Current Medical Evaluation\n",
    "CurrentEvaluation_FEATURES=['CDRSB','ADAS11','ADAS13','MMSE','RAVLT_learning','RAVLT_forgetting','RAVLT_perc_forgetting','RAVLT_immediate',\n",
    "                            'FAQ','MOCA','EcogPtLang','EcogPtVisspat','EcogPtPlan','EcogPtOrgan','EcogPtDivatt','EcogPtMem','EcogPtTotal',\n",
    "                            'EcogSPLang','EcogSPVisspat','EcogSPPlan','EcogSPOrgan','EcogSPDivatt','EcogSPMem','EcogSPTotal'];\n",
    "\n",
    "\n",
    "# Current Diagnosis\n",
    "CurrentDiagnosis_FEATURES= ['AD'];\n",
    "\n",
    "# Longitudinal Method\n",
    "LongitudinalMethod=2;\n",
    "MetricList=['MaxTime','Delta','Mean','Std'];\n",
    "\n",
    "# Run Longitudinal Data Anaysis\n",
    "LongitudinalDataAnalysis.runLongitudinal(InputToLongitudinal,OutputFromLongitudinal,Patient_FEATURES,Demo_FEATURES,\\\n",
    "                                         BaselineOneTime_FEATURES,Time_FEATURES,BaselineEvaluation_FEATURES,\\\n",
    "                                         CurrentEvaluation_FEATURES,CurrentDiagnosis_FEATURES,LongitudinalMethod,MetricList)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
