{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "d_meta = pd.read_csv('Toy_MetaClassifier.csv')\n",
    "labels = list(d_meta)\n",
    "data = d_meta.as_matrix()[:,:] \n",
    "\n",
    "Y = data[:, -1]\n",
    "X = data[:,0:-1]\n",
    "\n",
    "training_set_X, test_set_X, training_set_Y, test_set_Y, = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PET_and_MRI_length = 60\n",
    "Additional_cost_for_imaging = np.inf\n",
    "feature_costs = {'1':20, '2':20,'3':0,'4':2*60+247, '5':30, '6':45, '7':45,'8':10,'9':45,'10':10,'11':12,'12':10,'13':10}\n",
    "model_info = {}\n",
    "with open(\"Models_MetaClassifier.csv\", \"r\") as ins:\n",
    "    for line in ins:\n",
    "        info = line.rstrip('\\n').rstrip('\\r').split(\",\")\n",
    "        model_info[info[0]] = info[1:]\n",
    "\n",
    "global model_cost \n",
    "model_cost = []\n",
    "for i in labels[:-1]:\n",
    "    model_num = i[i.index('l')+1:]\n",
    "    model_string = 'Model '+str(model_num)\n",
    "    cost = 0\n",
    "    for feature in model_info[model_string]:\n",
    "        if feature != \"\":\n",
    "            cost += feature_costs[feature]\n",
    "    model_cost.append(cost)\n",
    "    \n",
    "global model_condition_cost \n",
    "model_condition_cost = {}\n",
    "for i in labels[:-1]:\n",
    "    model_num1 = i[i.index('l')+1:]\n",
    "    model_string = 'Model '+str(model_num1)\n",
    "    model_condition_cost[int(model_num1)] = {}\n",
    "    for j in labels[:-1]:\n",
    "        model_num2 = j[j.index('l')+1:]\n",
    "        model_string2 = 'Model '+str(model_num2)\n",
    "        \n",
    "        features1 = model_info[model_string]\n",
    "        features2 = model_info[model_string2]\n",
    "        additional_cost = 0\n",
    "        for f in features2:\n",
    "            if f not in features1:\n",
    "                additional_cost += feature_costs[f]\n",
    "                \n",
    "        model_condition_cost[int(model_num1)][int(model_num2)] = additional_cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionNode(object):\n",
    "    \"\"\"Makes Decision Node Class\"\"\"\n",
    "    def __init__(self,feature=None,left=None,right=None,classes = None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = feature\n",
    "        self.classes = classes\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.left == None and self.right == None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.feature == None:\n",
    "            return \"Decision Leaf with classes \" + str(self.classes)\n",
    "        return \"Decision node for feature \" + str(self.feature)\n",
    "\n",
    "class DecisionTree(object):\n",
    "    \"\"\"Decision Tree Class\"\"\"\n",
    "    def __init__(self):\n",
    "        self.root_node = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predicted_classes = []\n",
    "\n",
    "        for sample in X:\n",
    "            c = None\n",
    "            current_node = self.root_node\n",
    "            while c is None:\n",
    "                if current_node.is_leaf():\n",
    "                    try:\n",
    "                        c = int(current_node.classes)\n",
    "                    except:\n",
    "                        ones = sum(current_node.classes)\n",
    "                        zeros = len(current_node.classes)-ones\n",
    "                        if ones > zeros:\n",
    "                            c = 1\n",
    "                        else:\n",
    "                            c = 0\n",
    "                    \n",
    "                else:\n",
    "                    key_value = sample[current_node.feature]\n",
    "                    if key_value == 0:\n",
    "                        current_node = current_node.left\n",
    "                    elif key_value == 1:\n",
    "                        current_node = current_node.right\n",
    "            predicted_classes.append(c)\n",
    "        return predicted_classes\n",
    "    \n",
    "    def fit(self,samples,outcome_variable):\n",
    "        \"\"\"Takes in training data and builds a decision tree\n",
    "        samples = X values as list of list\n",
    "        outcome_varaible = Y value\n",
    "        \"\"\"\n",
    "        training_samples = [(s, t) for s, t in zip(samples, outcome_variable)]\n",
    "        predicting_features = list(range(len(samples[0])))\n",
    "        self.root_node = self.build_decision_tree(training_samples,predicting_features)\n",
    "        \n",
    "    def build_decision_tree(self,samples,features):\n",
    "        \n",
    "        classes = [sample[1] for sample in samples]\n",
    "        if len(set(classes)) == 1:\n",
    "            root_node = DecisionNode(feature=None, left=None, right=None, classes = classes[0])\n",
    "    \n",
    "        elif features == []:\n",
    "            return  DecisionNode(feature=None, left=None, right=None, classes = [sample[1] for sample in samples])\n",
    "\n",
    "        else:\n",
    "            best_feature = self.select_best_feature(samples,features,classes)\n",
    "            print \"BEST FEATURE THIS ITERATION IS \", best_feature\n",
    "            if best_feature == 'stop':\n",
    "                #print \"STOPPING\"\n",
    "                root_node = DecisionNode(feature=None, left=None, right=None, classes = [sample[1] for sample in samples])\n",
    "                return root_node\n",
    "            \n",
    "            best_feature_values = [s[0][best_feature] for s in samples]\n",
    "            if len(best_feature_values) == 1:\n",
    "                #print \"making leaf\"\n",
    "                root_node = DecisionNode(feature = best_feature, classes = best_feature_values[0])\n",
    "            else:\n",
    "                #do left hand side\n",
    "                left_samples = [s for s in samples if s[0][best_feature] == 0]\n",
    "                left_node = self.build_decision_tree(left_samples,features)\n",
    "                \n",
    "                #do right hand side\n",
    "                right_samples = [s for s in samples if s[0][best_feature] == 1]\n",
    "                right_node = self.build_decision_tree(right_samples,features)\n",
    "\n",
    "                root_node = DecisionNode(feature = best_feature, classes = best_feature_values, left = left_node, right= right_node)\n",
    "\n",
    "        return root_node\n",
    "    \n",
    "    \n",
    "    def print_tree(self):\n",
    "        curr_node = self.root_node\n",
    "        print self.__str__(curr_node,0)\n",
    "\n",
    "    def __str__(self, node, depth=0):\n",
    "        ret = \"\"\n",
    "        # Print right branch\n",
    "        if node.right != None:\n",
    "            ret += self.__str__(node.right,depth + 1)\n",
    "        # Print own value\n",
    "        if node.feature != None:\n",
    "            ret += \"\\n\" + (\"    \"*depth) + str(node.feature)\n",
    "        else:\n",
    "            ret += \"\\n\" + (\"    \"*depth) #+ \"Class: \"+str(node.classes)\n",
    "        # Print left branch\n",
    "        if node.left != None:\n",
    "            ret += self.__str__(node.left,depth + 1)\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def select_best_feature(self, samples, features, classes):\n",
    "        \"\"\"\n",
    "        Find score for all remaining features, choose the one that maximizes\n",
    "        the score function and delete this feature from consideration\n",
    "        \"\"\"\n",
    "        gain_factors = [(self.score_function(samples, feat, classes, features), feat)\n",
    "                        for feat in features]\n",
    "        gain_factors.sort()\n",
    "        print \"GAIN FACTORS\", gain_factors\n",
    "        best_feature = gain_factors[-1][1]\n",
    "        if gain_factors[-1][0] <= 0:\n",
    "            return \"stop\"\n",
    "        features.pop(features.index(best_feature))\n",
    "        return best_feature\n",
    "\n",
    "\n",
    "    def information_gain(self, samples, feature, classes):\n",
    "        \"\"\"\n",
    "        Information gain is the measure of the difference in entropy from before\n",
    "        to after the samples are split on the given feature values. In other\n",
    "        words, how much uncertainty in the samples was reduced after splitting\n",
    "        them on the given feature.\n",
    "        \"\"\"\n",
    "        #print \"splitting by\", feature\n",
    "        N = len(samples)\n",
    "        samples_partition = defaultdict(list)\n",
    "        for s in samples:\n",
    "            samples_partition[s[0][feature]].append(s)\n",
    "        feature_entropy = 0.0\n",
    "        for partition in samples_partition.values():\n",
    "            sub_classes = [s[1] for s in partition]\n",
    "            feature_entropy += (len(partition) / float(N)) * self.entropy(sub_classes)\n",
    "        #print \"Child is\", feature_entropy\n",
    "        p = self.entropy(classes)\n",
    "        #print \"Parent is\", p\n",
    "        return p, feature_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(dataset):\n",
    "        \"\"\"Measure of the amount of uncertainty in the given dataset.\"\"\"\n",
    "\n",
    "        N = len(dataset)\n",
    "        counter = Counter(dataset)\n",
    "        #rint counter\n",
    "        return sum([-1.0*(counter[k] / float(N))*math.log(counter[k] / float(N),2) for k in counter])\n",
    "    \n",
    "    def cost(self, feature):\n",
    "        global model_cost\n",
    "        return model_cost[feature]\n",
    "        #return c[feature]\n",
    "    \n",
    "    def cost_conditional(self, feature, feature2):\n",
    "        global model_condition_cost\n",
    "        return model_condition_cost[feature][feature2]\n",
    "    \n",
    "    def P_L(self, samples, feature, classes):\n",
    "        parent_entrop, child_entrop = self.information_gain(samples, feature, classes)\n",
    "        return 1- (child_entrop)/float(parent_entrop)\n",
    "    \n",
    "    def P_L_conditional(self, samples, feature, classes, features_left):\n",
    "        left = [i for i in samples if i[0][feature] == 0]\n",
    "        right = [i for i in samples if i[0][feature] == 1]\n",
    "        \n",
    "        hypothetical_featuresleft = [i for i in features_left]\n",
    "        hypothetical_featuresleft.pop(hypothetical_featuresleft.index(feature))\n",
    "        \n",
    "        sums = 0\n",
    "        #left split\n",
    "        if left != []:\n",
    "            for j in hypothetical_featuresleft:\n",
    "                pl = self.P_L(left, j, classes)\n",
    "                sums += pl*self.cost_conditional(feature,j)\n",
    "                \n",
    "        #right split\n",
    "        if right != []:\n",
    "            for j in hypothetical_featuresleft:\n",
    "                pl = self.P_L(right, j, classes)\n",
    "                sums += pl*self.cost_conditional(feature,j)\n",
    "                \n",
    "        return sums\n",
    "    \n",
    "    \n",
    "    def Estimated_Cost(self, samples, feature, classes, features_left):\n",
    "        PL = self.P_L(samples, feature, classes)\n",
    "        if PL > 1:\n",
    "            print \"PLLLLL\", PL\n",
    "        if PL == 1:\n",
    "            return PL*self.cost(feature)\n",
    "        first_part = PL*self.cost(feature)\n",
    "        second_part = (1-PL)*self.P_L_conditional(samples, feature, classes, features_left) #need_features_left\n",
    "        #print \"2nd\",second_part\n",
    "        return first_part + second_part\n",
    "    \n",
    "    \n",
    "    \n",
    "    def score_function(self,samples, feature, classes, features_left):\n",
    "        #print \"FEATURE IS\", feature\n",
    "        info_gain1, info_gain2 = self.information_gain(samples, feature, classes)\n",
    "        #print \"         Info gain is\",info_gain1-info_gain2\n",
    "        estimated_cost = self.Estimated_Cost(samples, feature, classes, features_left)\n",
    "        if estimated_cost == 0:\n",
    "            return 0\n",
    "        print \"Estimated cost for feature\",feature, \"is \", estimated_cost\n",
    "        #print \"Score is \", (info_gain1-info_gain2)/estimated_cost\n",
    "        return (info_gain1-info_gain2)/float((estimated_cost))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Function to print the Decision Tree Classifier from sklearn to compare results \"\"\"\n",
    "def get_code(tree, feature_names):\n",
    "        left      = tree.tree_.children_left\n",
    "        right     = tree.tree_.children_right\n",
    "        threshold = tree.tree_.threshold\n",
    "        features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "        value = tree.tree_.value\n",
    "\n",
    "        def recurse(left, right, threshold, features, node):\n",
    "                if (threshold[node] != -2):\n",
    "                        print \"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\"\n",
    "                        if left[node] != -1:\n",
    "                                recurse (left, right, threshold, features,left[node])\n",
    "                        print \"} else {\"\n",
    "                        if right[node] != -1:\n",
    "                                recurse (left, right, threshold, features,right[node])\n",
    "                        print \"}\"\n",
    "                else:\n",
    "                        print \"return \" + str(value[node])\n",
    "\n",
    "        recurse(left, right, threshold, features, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST FEATURE THIS ITERATION IS  1\n",
      "BEST FEATURE THIS ITERATION IS  4\n",
      "BEST FEATURE THIS ITERATION IS  stop\n",
      "\n",
      "    Class: 0\n",
      "1\n",
      "        Class: 1\n",
      "    4\n",
      "        Class: [1, 0, 1]\n",
      "\n",
      "Sklearn tree\n",
      "if ( 1 <= 0.5 ) {\n",
      "if ( 4 <= 0.5 ) {\n",
      "return [[ 1.  2.]]\n",
      "} else {\n",
      "return [[ 0.  1.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 1.  0.]]\n",
      "}\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = np.array([[0,0,1,0,1],[1,1,1,1,0],[1,0,0,1,0],[1,0,0,1,0],[1,0,0,1,0]])\n",
    "targets = np.array([1,0,1,0,1])\n",
    "\n",
    "#samples = [[0,0,0,0,1],[1,1,1,1,0],[0,0,0,1,0],[1,0,0,1,0]]\n",
    "#targets = [1,0,0,1]\n",
    "\n",
    "#samples = [[0,0,1,0,1],[1,1,1,1,0],[0,0,0,1,0],[1,0,0,1,0]]\n",
    "#targets = [1,0,1,1]\n",
    "\n",
    "#samples = [[0,0,1,0,1],[1,1,1,1,0],[0,0,0,1,1],[1,0,0,1,0],[0,1,1,1,0],[1,1,0,0,0]]\n",
    "#targets = [1,0,1,1,0,1]\n",
    "\n",
    "#samples = [[1,1,1,0,1],[1,1,0,1,0],[0,0,0,1,1],[0,0,0,1,0],[0,1,1,1,0],[1,1,0,0,0]]\n",
    "#targets = [1,0,1,0,0,1]\n",
    "\n",
    "\n",
    "d = DecisionTree()\n",
    "d.fit(samples,targets)\n",
    "d.print_tree()\n",
    "\n",
    "print \"\\nSklearn tree\"\n",
    "dtree = tree.DecisionTreeClassifier(criterion = 'entropy')\n",
    "dtree = dtree.fit(samples,targets)\n",
    "get_code(dtree,['0','1','2','3','4','5'])\n",
    "\n",
    "\n",
    "print d.predict([[0,0,0,1,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if ( 306 <= 0.5 ) {\n",
      "if ( 49 <= 0.5 ) {\n",
      "if ( 295 <= 0.5 ) {\n",
      "if ( 25 <= 0.5 ) {\n",
      "return [[ 129.    0.]]\n",
      "} else {\n",
      "if ( 91 <= 0.5 ) {\n",
      "if ( 157 <= 0.5 ) {\n",
      "if ( 34 <= 0.5 ) {\n",
      "if ( 14 <= 0.5 ) {\n",
      "if ( 158 <= 0.5 ) {\n",
      "if ( 209 <= 0.5 ) {\n",
      "if ( 13 <= 0.5 ) {\n",
      "return [[ 13.   1.]]\n",
      "} else {\n",
      "return [[ 1.  0.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 1.  0.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 2.  0.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 2.  0.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 3.  0.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 10.   0.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 41.   0.]]\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( 98 <= 0.5 ) {\n",
      "return [[ 2.  0.]]\n",
      "} else {\n",
      "return [[ 0.  1.]]\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( 17 <= 0.5 ) {\n",
      "if ( 218 <= 0.5 ) {\n",
      "if ( 100 <= 0.5 ) {\n",
      "return [[ 3.  0.]]\n",
      "} else {\n",
      "return [[ 0.  3.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 11.   0.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 0.  4.]]\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( 291 <= 0.5 ) {\n",
      "if ( 362 <= 0.5 ) {\n",
      "if ( 169 <= 0.5 ) {\n",
      "return [[ 5.  0.]]\n",
      "} else {\n",
      "if ( 38 <= 0.5 ) {\n",
      "return [[ 0.  1.]]\n",
      "} else {\n",
      "return [[ 1.  0.]]\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( 195 <= 0.5 ) {\n",
      "if ( 329 <= 0.5 ) {\n",
      "return [[ 0.  4.]]\n",
      "} else {\n",
      "return [[ 5.  0.]]\n",
      "}\n",
      "} else {\n",
      "if ( 23 <= 0.5 ) {\n",
      "if ( 7 <= 0.5 ) {\n",
      "if ( 25 <= 0.5 ) {\n",
      "return [[ 0.  2.]]\n",
      "} else {\n",
      "return [[ 1.  0.]]\n",
      "}\n",
      "} else {\n",
      "return [[  0.  17.]]\n",
      "}\n",
      "} else {\n",
      "return [[ 1.  0.]]\n",
      "}\n",
      "}\n",
      "}\n",
      "} else {\n",
      "if ( 346 <= 0.5 ) {\n",
      "if ( 111 <= 0.5 ) {\n",
      "return [[ 0.  2.]]\n",
      "} else {\n",
      "return [[ 2.  0.]]\n",
      "}\n",
      "} else {\n",
      "return [[   0.  116.]]\n",
      "}\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dtree = tree.DecisionTreeClassifier(criterion = 'entropy')\n",
    "\n",
    "dtree = dtree.fit(training_set_X,training_set_Y)\n",
    "get_code(dtree,[str(i) for i in range(8191)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST FEATURE THIS ITERATION IS  346\n",
      "BEST FEATURE THIS ITERATION IS  13\n",
      "BEST FEATURE THIS ITERATION IS  323\n",
      "BEST FEATURE THIS ITERATION IS  14\n",
      "BEST FEATURE THIS ITERATION IS  4\n",
      "BEST FEATURE THIS ITERATION IS  370\n",
      "BEST FEATURE THIS ITERATION IS  345\n",
      "BEST FEATURE THIS ITERATION IS  248\n",
      "BEST FEATURE THIS ITERATION IS  25\n",
      "BEST FEATURE THIS ITERATION IS  10\n",
      "BEST FEATURE THIS ITERATION IS  3\n",
      "BEST FEATURE THIS ITERATION IS  22\n",
      "BEST FEATURE THIS ITERATION IS  102\n",
      "BEST FEATURE THIS ITERATION IS  165\n",
      "BEST FEATURE THIS ITERATION IS  238\n",
      "BEST FEATURE THIS ITERATION IS  254\n",
      "BEST FEATURE THIS ITERATION IS  173\n",
      "BEST FEATURE THIS ITERATION IS  7\n",
      "BEST FEATURE THIS ITERATION IS  stop\n",
      "BEST FEATURE THIS ITERATION IS  266\n",
      "BEST FEATURE THIS ITERATION IS  157\n",
      "BEST FEATURE THIS ITERATION IS  109\n",
      "BEST FEATURE THIS ITERATION IS  365\n",
      "BEST FEATURE THIS ITERATION IS  360\n",
      "BEST FEATURE THIS ITERATION IS  376\n",
      "BEST FEATURE THIS ITERATION IS  15\n",
      "BEST FEATURE THIS ITERATION IS  99\n",
      "BEST FEATURE THIS ITERATION IS  36\n",
      "BEST FEATURE THIS ITERATION IS  284\n",
      "BEST FEATURE THIS ITERATION IS  372\n",
      "BEST FEATURE THIS ITERATION IS  163\n",
      "BEST FEATURE THIS ITERATION IS  320\n",
      "BEST FEATURE THIS ITERATION IS  217\n",
      "BEST FEATURE THIS ITERATION IS  183\n",
      "BEST FEATURE THIS ITERATION IS  352\n",
      "BEST FEATURE THIS ITERATION IS  195\n",
      "BEST FEATURE THIS ITERATION IS  137\n",
      "BEST FEATURE THIS ITERATION IS  285\n",
      "BEST FEATURE THIS ITERATION IS  83\n",
      "BEST FEATURE THIS ITERATION IS  295\n",
      "BEST FEATURE THIS ITERATION IS  292\n",
      "BEST FEATURE THIS ITERATION IS  196\n",
      "BEST FEATURE THIS ITERATION IS  368\n",
      "BEST FEATURE THIS ITERATION IS  154\n",
      "BEST FEATURE THIS ITERATION IS  219\n",
      "BEST FEATURE THIS ITERATION IS  373\n",
      "BEST FEATURE THIS ITERATION IS  255\n",
      "\n",
      "                \n",
      "            292\n",
      "                                \n",
      "                            255\n",
      "                                \n",
      "                        219\n",
      "                                \n",
      "                            373\n",
      "                                \n",
      "                    154\n",
      "                        \n",
      "                196\n",
      "                        \n",
      "                    368\n",
      "                        \n",
      "        83\n",
      "                \n",
      "            295\n",
      "                \n",
      "    15\n",
      "                        \n",
      "                    195\n",
      "                                \n",
      "                            285\n",
      "                                \n",
      "                        137\n",
      "                            \n",
      "                183\n",
      "                        \n",
      "                    352\n",
      "                        \n",
      "            163\n",
      "                    \n",
      "                320\n",
      "                        \n",
      "                    217\n",
      "                        \n",
      "        99\n",
      "                    \n",
      "                284\n",
      "                        \n",
      "                    372\n",
      "                        \n",
      "            36\n",
      "                \n",
      "346\n",
      "                    \n",
      "                376\n",
      "                    \n",
      "            360\n",
      "                \n",
      "        157\n",
      "                    \n",
      "                365\n",
      "                    \n",
      "            109\n",
      "                \n",
      "    13\n",
      "            \n",
      "        323\n",
      "                    \n",
      "                266\n",
      "                    \n",
      "            14\n",
      "                    \n",
      "                4\n",
      "                        \n",
      "                    370\n",
      "                            \n",
      "                        345\n",
      "                                \n",
      "                            248\n",
      "                                    \n",
      "                                25\n",
      "                                        \n",
      "                                    10\n",
      "                                            \n",
      "                                        3\n",
      "                                                \n",
      "                                            22\n",
      "                                                    \n",
      "                                                102\n",
      "                                                        \n",
      "                                                    165\n",
      "                                                            \n",
      "                                                        238\n",
      "                                                                \n",
      "                                                            254\n",
      "                                                                    \n",
      "                                                                173\n",
      "                                                                        \n",
      "                                                                    7\n",
      "                                                                        \n",
      "0.910526315789\n",
      "0.851351351351\n"
     ]
    }
   ],
   "source": [
    "d = DecisionTree()\n",
    "d.fit(training_set_X,training_set_Y)\n",
    "d.print_tree()\n",
    "\n",
    "y_pred = d.predict(test_set_X)\n",
    "\n",
    "count = []\n",
    "recall = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == test_set_Y[i]:\n",
    "        count.append(1)\n",
    "    else:\n",
    "        count.append(0)\n",
    "        \n",
    "    if test_set_Y[i] == 1:\n",
    "        if y_pred[i] == 1:\n",
    "            recall.append(1)\n",
    "        else:\n",
    "            recall.append(0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910526315789\n",
      "0.851351351351\n"
     ]
    }
   ],
   "source": [
    "print sum(count)/float(len(count))\n",
    "print sum(recall)/float(len(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST FEATURE THIS ITERATION IS  306\n",
      "BEST FEATURE THIS ITERATION IS  164\n",
      "BEST FEATURE THIS ITERATION IS  236\n",
      "BEST FEATURE THIS ITERATION IS  14\n",
      "BEST FEATURE THIS ITERATION IS  91\n",
      "BEST FEATURE THIS ITERATION IS  3\n",
      "BEST FEATURE THIS ITERATION IS  22\n",
      "BEST FEATURE THIS ITERATION IS  11\n",
      "BEST FEATURE THIS ITERATION IS  10\n",
      "BEST FEATURE THIS ITERATION IS  13\n",
      "BEST FEATURE THIS ITERATION IS  7\n",
      "BEST FEATURE THIS ITERATION IS  254\n",
      "BEST FEATURE THIS ITERATION IS  4\n",
      "BEST FEATURE THIS ITERATION IS  165\n",
      "BEST FEATURE THIS ITERATION IS  stop\n",
      "BEST FEATURE THIS ITERATION IS  280\n",
      "BEST FEATURE THIS ITERATION IS  102\n",
      "BEST FEATURE THIS ITERATION IS  248\n",
      "BEST FEATURE THIS ITERATION IS  107\n",
      "BEST FEATURE THIS ITERATION IS  235\n",
      "BEST FEATURE THIS ITERATION IS  15\n",
      "BEST FEATURE THIS ITERATION IS  141\n",
      "BEST FEATURE THIS ITERATION IS  314\n",
      "BEST FEATURE THIS ITERATION IS  42\n",
      "BEST FEATURE THIS ITERATION IS  246\n",
      "BEST FEATURE THIS ITERATION IS  48\n",
      "BEST FEATURE THIS ITERATION IS  1\n",
      "BEST FEATURE THIS ITERATION IS  204\n",
      "BEST FEATURE THIS ITERATION IS  256\n",
      "BEST FEATURE THIS ITERATION IS  32\n",
      "BEST FEATURE THIS ITERATION IS  44\n",
      "BEST FEATURE THIS ITERATION IS  99\n",
      "BEST FEATURE THIS ITERATION IS  109\n",
      "BEST FEATURE THIS ITERATION IS  174\n",
      "BEST FEATURE THIS ITERATION IS  349\n",
      "BEST FEATURE THIS ITERATION IS  255\n",
      "BEST FEATURE THIS ITERATION IS  361\n",
      "BEST FEATURE THIS ITERATION IS  375\n",
      "BEST FEATURE THIS ITERATION IS  86\n",
      "BEST FEATURE THIS ITERATION IS  25\n",
      "BEST FEATURE THIS ITERATION IS  39\n",
      "\n",
      "            \n",
      "        174\n",
      "                        \n",
      "                    39\n",
      "                        \n",
      "                375\n",
      "                        \n",
      "                    86\n",
      "                            \n",
      "                        25\n",
      "                            \n",
      "            349\n",
      "                        \n",
      "                    361\n",
      "                        \n",
      "                255\n",
      "                    \n",
      "    15\n",
      "                    \n",
      "                256\n",
      "                            \n",
      "                        99\n",
      "                                \n",
      "                            109\n",
      "                                \n",
      "                    32\n",
      "                            \n",
      "                        44\n",
      "                            \n",
      "            48\n",
      "                        \n",
      "                    204\n",
      "                        \n",
      "                1\n",
      "                    \n",
      "        141\n",
      "                        \n",
      "                    246\n",
      "                        \n",
      "                42\n",
      "                    \n",
      "            314\n",
      "                \n",
      "306\n",
      "                \n",
      "            235\n",
      "                \n",
      "        280\n",
      "                        \n",
      "                    107\n",
      "                        \n",
      "                248\n",
      "                    \n",
      "            102\n",
      "                \n",
      "    164\n",
      "            \n",
      "        236\n",
      "                \n",
      "            14\n",
      "                    \n",
      "                91\n",
      "                        \n",
      "                    3\n",
      "                            \n",
      "                        22\n",
      "                                \n",
      "                            11\n",
      "                                    \n",
      "                                10\n",
      "                                        \n",
      "                                    13\n",
      "                                            \n",
      "                                        7\n",
      "                                                \n",
      "                                            254\n",
      "                                                    \n",
      "                                                4\n",
      "                                                        \n",
      "                                                    165\n",
      "                                                        \n"
     ]
    }
   ],
   "source": [
    "d = DecisionTree()\n",
    "d.fit(training_set_X,training_set_Y)\n",
    "d.print_tree()\n",
    "\n",
    "y_pred = d.predict(test_set_X)\n",
    "\n",
    "count = []\n",
    "recall = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == test_set_Y[i]:\n",
    "        count.append(1)\n",
    "    else:\n",
    "        count.append(0)\n",
    "        \n",
    "    if test_set_Y[i] == 1:\n",
    "        if y_pred[i] == 1:\n",
    "            recall.append(1)\n",
    "        else:\n",
    "            recall.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910526315789\n",
      "0.86301369863\n"
     ]
    }
   ],
   "source": [
    "print sum(count)/float(len(count))\n",
    "print sum(recall)/float(len(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST FEATURE THIS ITERATION IS  258\n",
      "BEST FEATURE THIS ITERATION IS  299\n",
      "BEST FEATURE THIS ITERATION IS  1\n",
      "BEST FEATURE THIS ITERATION IS  276\n",
      "BEST FEATURE THIS ITERATION IS  10\n",
      "BEST FEATURE THIS ITERATION IS  14\n",
      "BEST FEATURE THIS ITERATION IS  3\n",
      "BEST FEATURE THIS ITERATION IS  13\n",
      "BEST FEATURE THIS ITERATION IS  22\n",
      "BEST FEATURE THIS ITERATION IS  11\n",
      "BEST FEATURE THIS ITERATION IS  260\n",
      "BEST FEATURE THIS ITERATION IS  257\n",
      "BEST FEATURE THIS ITERATION IS  285\n",
      "BEST FEATURE THIS ITERATION IS  7\n",
      "BEST FEATURE THIS ITERATION IS  209\n",
      "BEST FEATURE THIS ITERATION IS  stop\n",
      "BEST FEATURE THIS ITERATION IS  25\n",
      "BEST FEATURE THIS ITERATION IS  40\n",
      "BEST FEATURE THIS ITERATION IS  224\n",
      "BEST FEATURE THIS ITERATION IS  30\n",
      "BEST FEATURE THIS ITERATION IS  0\n",
      "BEST FEATURE THIS ITERATION IS  91\n",
      "BEST FEATURE THIS ITERATION IS  26\n",
      "BEST FEATURE THIS ITERATION IS  165\n",
      "BEST FEATURE THIS ITERATION IS  177\n",
      "BEST FEATURE THIS ITERATION IS  164\n",
      "BEST FEATURE THIS ITERATION IS  stop\n",
      "BEST FEATURE THIS ITERATION IS  173\n",
      "BEST FEATURE THIS ITERATION IS  284\n",
      "BEST FEATURE THIS ITERATION IS  373\n",
      "BEST FEATURE THIS ITERATION IS  212\n",
      "BEST FEATURE THIS ITERATION IS  45\n",
      "BEST FEATURE THIS ITERATION IS  42\n",
      "BEST FEATURE THIS ITERATION IS  157\n",
      "BEST FEATURE THIS ITERATION IS  289\n",
      "BEST FEATURE THIS ITERATION IS  15\n",
      "BEST FEATURE THIS ITERATION IS  283\n",
      "BEST FEATURE THIS ITERATION IS  369\n",
      "BEST FEATURE THIS ITERATION IS  199\n",
      "BEST FEATURE THIS ITERATION IS  363\n",
      "BEST FEATURE THIS ITERATION IS  74\n",
      "BEST FEATURE THIS ITERATION IS  105\n",
      "BEST FEATURE THIS ITERATION IS  302\n",
      "BEST FEATURE THIS ITERATION IS  362\n",
      "BEST FEATURE THIS ITERATION IS  53\n",
      "BEST FEATURE THIS ITERATION IS  265\n",
      "BEST FEATURE THIS ITERATION IS  9\n",
      "BEST FEATURE THIS ITERATION IS  120\n",
      "BEST FEATURE THIS ITERATION IS  117\n",
      "BEST FEATURE THIS ITERATION IS  36\n",
      "\n",
      "                \n",
      "            53\n",
      "                        \n",
      "                    120\n",
      "                                \n",
      "                            36\n",
      "                                \n",
      "                        117\n",
      "                            \n",
      "                265\n",
      "                        \n",
      "                    9\n",
      "                        \n",
      "        302\n",
      "                \n",
      "            362\n",
      "                \n",
      "    157\n",
      "                    \n",
      "                74\n",
      "                        \n",
      "                    105\n",
      "                        \n",
      "            199\n",
      "                    \n",
      "                363\n",
      "                    \n",
      "        289\n",
      "                \n",
      "            15\n",
      "                        \n",
      "                    369\n",
      "                        \n",
      "                283\n",
      "                    \n",
      "258\n",
      "            \n",
      "        42\n",
      "            \n",
      "    299\n",
      "                        \n",
      "                    45\n",
      "                        \n",
      "                224\n",
      "                            \n",
      "                        212\n",
      "                            \n",
      "                    30\n",
      "                            \n",
      "                        0\n",
      "                                            \n",
      "                                        373\n",
      "                                            \n",
      "                                    284\n",
      "                                        \n",
      "                                173\n",
      "                                    \n",
      "                            91\n",
      "                                    \n",
      "                                26\n",
      "                                        \n",
      "                                    165\n",
      "                                            \n",
      "                                        177\n",
      "                                                \n",
      "                                            164\n",
      "                                                \n",
      "            25\n",
      "                    \n",
      "                40\n",
      "                    \n",
      "        1\n",
      "                \n",
      "            276\n",
      "                    \n",
      "                10\n",
      "                        \n",
      "                    14\n",
      "                            \n",
      "                        3\n",
      "                                \n",
      "                            13\n",
      "                                    \n",
      "                                22\n",
      "                                        \n",
      "                                    11\n",
      "                                            \n",
      "                                        260\n",
      "                                                \n",
      "                                            257\n",
      "                                                    \n",
      "                                                285\n",
      "                                                        \n",
      "                                                    7\n",
      "                                                            \n",
      "                                                        209\n",
      "                                                            \n"
     ]
    }
   ],
   "source": [
    "d = DecisionTree()\n",
    "d.fit(training_set_X,training_set_Y)\n",
    "d.print_tree()\n",
    "\n",
    "y_pred = d.predict(test_set_X)\n",
    "\n",
    "count = []\n",
    "recall = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == test_set_Y[i]:\n",
    "        count.append(1)\n",
    "    else:\n",
    "        count.append(0)\n",
    "        \n",
    "    if test_set_Y[i] == 1:\n",
    "        if y_pred[i] == 1:\n",
    "            recall.append(1)\n",
    "        else:\n",
    "            recall.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910526315789\n",
      "0.835616438356\n"
     ]
    }
   ],
   "source": [
    "print sum(count)/float(len(count))\n",
    "print sum(recall)/float(len(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost for feature 0 is  47099.6497169\n",
      "Estimated cost for feature 1 is  44726.2990188\n",
      "Estimated cost for feature 2 is  52199.2599024\n",
      "Estimated cost for feature 3 is  16859.2687327\n",
      "Estimated cost for feature 4 is  27591.0873983\n",
      "Estimated cost for feature 5 is  27257.4418935\n",
      "Estimated cost for feature 6 is  28387.4979354\n",
      "Estimated cost for feature 7 is  30459.7253204\n",
      "Estimated cost for feature 8 is  31183.549925\n",
      "Estimated cost for feature 9 is  30373.6924613\n",
      "Estimated cost for feature 10 is  39536.633574\n",
      "Estimated cost for feature 11 is  42672.7965791\n",
      "Estimated cost for feature 12 is  43271.1352672\n",
      "Estimated cost for feature 13 is  44371.5610347\n",
      "Estimated cost for feature 14 is  47265.9529668\n",
      "Estimated cost for feature 15 is  16145.4457565\n",
      "Estimated cost for feature 16 is  24193.0427776\n",
      "Estimated cost for feature 17 is  26359.5873791\n",
      "Estimated cost for feature 18 is  26172.530561\n",
      "Estimated cost for feature 19 is  28638.8333621\n",
      "Estimated cost for feature 20 is  29770.2997045\n",
      "Estimated cost for feature 21 is  28191.5018349\n",
      "Estimated cost for feature 22 is  40089.4738613\n",
      "Estimated cost for feature 23 is  40391.8837973\n",
      "Estimated cost for feature 24 is  41922.7709516\n",
      "Estimated cost for feature 25 is  44932.0165312\n",
      "Estimated cost for feature 26 is  14335.5551298\n",
      "Estimated cost for feature 27 is  25448.3398876\n",
      "Estimated cost for feature 28 is  26340.9386813\n",
      "Estimated cost for feature 29 is  26352.1782085\n",
      "Estimated cost for feature 30 is  30212.9151718\n",
      "Estimated cost for feature 31 is  30010.3164452\n",
      "Estimated cost for feature 32 is  27286.9675175\n",
      "Estimated cost for feature 33 is  37678.5339382\n",
      "Estimated cost for feature 34 is  38909.4553106\n",
      "Estimated cost for feature 35 is  42360.5255084\n",
      "Estimated cost for feature 36 is  16660.5631264\n",
      "Estimated cost for feature 37 is  26336.4436116\n",
      "Estimated cost for feature 38 is  26359.5597004\n",
      "Estimated cost for feature 39 is  28256.1729095\n",
      "Estimated cost for feature 40 is  28881.2874218\n",
      "Estimated cost for feature 41 is  31312.3846335\n",
      "Estimated cost for feature 42 is  29991.687868\n",
      "Estimated cost for feature 43 is  38947.2017071\n",
      "Estimated cost for feature 44 is  42242.0701828\n",
      "Estimated cost for feature 45 is  43271.1352672\n",
      "Estimated cost for feature 46 is  9054.61879441\n",
      "Estimated cost for feature 47 is  9564.45649472\n",
      "Estimated cost for feature 48 is  9765.24633454\n",
      "Estimated cost for feature 49 is  11918.5022271\n",
      "Estimated cost for feature 50 is  10382.9993365\n",
      "Estimated cost for feature 51 is  11850.2061393\n",
      "Estimated cost for feature 52 is  14906.082909\n",
      "Estimated cost for feature 53 is  16839.2310703\n",
      "Estimated cost for feature 54 is  18192.5474536\n",
      "Estimated cost for feature 55 is  21271.7322265\n",
      "Estimated cost for feature 56 is  20871.2417075\n",
      "Estimated cost for feature 57 is  24073.5872957\n",
      "Estimated cost for feature 58 is  21787.6202721\n",
      "Estimated cost for feature 59 is  24460.4906464\n",
      "Estimated cost for feature 60 is  24758.0416019\n",
      "Estimated cost for feature 61 is  24566.9730646\n",
      "Estimated cost for feature 62 is  26310.3986022\n",
      "Estimated cost for feature 63 is  23879.6024635\n",
      "Estimated cost for feature 64 is  24863.4564889\n",
      "Estimated cost for feature 65 is  25206.5952788\n",
      "Estimated cost for feature 66 is  24289.1929708\n",
      "Estimated cost for feature 67 is  27202.9741468\n",
      "Estimated cost for feature 68 is  27254.6656254\n",
      "Estimated cost for feature 69 is  29363.8185707\n",
      "Estimated cost for feature 70 is  25126.3141674\n",
      "Estimated cost for feature 71 is  25967.3298065\n",
      "Estimated cost for feature 72 is  24801.636666\n",
      "Estimated cost for feature 73 is  26450.1166922\n",
      "Estimated cost for feature 74 is  28613.3261166\n",
      "Estimated cost for feature 75 is  28918.8525459\n",
      "Estimated cost for feature 76 is  27722.4647473\n",
      "Estimated cost for feature 77 is  26628.6929672\n",
      "Estimated cost for feature 78 is  29597.4837044\n",
      "Estimated cost for feature 79 is  28454.4239803\n",
      "Estimated cost for feature 80 is  28133.0262262\n",
      "Estimated cost for feature 81 is  25430.1411718\n",
      "Estimated cost for feature 82 is  28865.6875507\n",
      "Estimated cost for feature 83 is  30127.2236697\n",
      "Estimated cost for feature 84 is  29238.0107723\n",
      "Estimated cost for feature 85 is  29042.9988931\n",
      "Estimated cost for feature 86 is  29235.3457493\n",
      "Estimated cost for feature 87 is  31291.3380667\n",
      "Estimated cost for feature 88 is  40618.3418584\n",
      "Estimated cost for feature 89 is  41590.18947\n",
      "Estimated cost for feature 90 is  42974.5225047\n",
      "Estimated cost for feature 91 is  44504.0097723\n",
      "Estimated cost for feature 92 is  13766.1046434\n",
      "Estimated cost for feature 93 is  23448.8572367\n",
      "Estimated cost for feature 94 is  25188.5265703\n",
      "Estimated cost for feature 95 is  26615.710926\n",
      "Estimated cost for feature 96 is  28245.3871274\n",
      "Estimated cost for feature 97 is  28545.1890175\n",
      "Estimated cost for feature 98 is  23681.483433\n",
      "Estimated cost for feature 99 is  37167.6351045\n",
      "Estimated cost for feature 100 is  38276.3891598\n",
      "Estimated cost for feature 101 is  41125.1832248\n",
      "Estimated cost for feature 102 is  16249.1349295\n",
      "Estimated cost for feature 103 is  23454.4708879\n",
      "Estimated cost for feature 104 is  26828.7283658\n",
      "Estimated cost for feature 105 is  27335.9283834\n",
      "Estimated cost for feature 106 is  28754.6569591\n",
      "Estimated cost for feature 107 is  30385.598445\n",
      "Estimated cost for feature 108 is  28595.2746655\n",
      "Estimated cost for feature 109 is  40249.9093361\n",
      "Estimated cost for feature 110 is  38619.8908048\n",
      "Estimated cost for feature 111 is  41854.2373989\n",
      "Estimated cost for feature 112 is  8481.76856661\n",
      "Estimated cost for feature 113 is  8796.19573221\n",
      "Estimated cost for feature 114 is  8841.40005117\n",
      "Estimated cost for feature 115 is  11274.2656609\n",
      "Estimated cost for feature 116 is  10098.169522\n",
      "Estimated cost for feature 117 is  10003.4348875\n",
      "Estimated cost for feature 118 is  13725.3854693\n",
      "Estimated cost for feature 119 is  15925.372602\n",
      "Estimated cost for feature 120 is  16952.0864792\n",
      "Estimated cost for feature 121 is  20481.4352681\n",
      "Estimated cost for feature 122 is  19711.7229869\n",
      "Estimated cost for feature 123 is  23076.2550297\n",
      "Estimated cost for feature 124 is  20348.1256476\n",
      "Estimated cost for feature 125 is  22649.9145323\n",
      "Estimated cost for feature 126 is  23465.5154951\n",
      "Estimated cost for feature 127 is  23493.9312789\n",
      "Estimated cost for feature 128 is  25987.5773146\n",
      "Estimated cost for feature 129 is  22708.3943838\n",
      "Estimated cost for feature 130 is  24745.6271424\n",
      "Estimated cost for feature 131 is  24111.4566813\n",
      "Estimated cost for feature 132 is  22576.0490887\n",
      "Estimated cost for feature 133 is  25326.8277559\n",
      "Estimated cost for feature 134 is  27747.8227423\n",
      "Estimated cost for feature 135 is  29606.5936337\n",
      "Estimated cost for feature 136 is  25101.554012\n",
      "Estimated cost for feature 137 is  25234.7450836\n",
      "Estimated cost for feature 138 is  23739.1150251\n",
      "Estimated cost for feature 139 is  24685.4307784\n",
      "Estimated cost for feature 140 is  27799.9321362\n",
      "Estimated cost for feature 141 is  29337.351774\n",
      "Estimated cost for feature 142 is  26423.2299549\n",
      "Estimated cost for feature 143 is  24786.6268673\n",
      "Estimated cost for feature 144 is  28901.494493\n",
      "Estimated cost for feature 145 is  28114.8950335\n",
      "Estimated cost for feature 146 is  28402.9184649\n",
      "Estimated cost for feature 147 is  24067.0356689\n",
      "Estimated cost for feature 148 is  28612.8529852\n",
      "Estimated cost for feature 149 is  28781.9808954\n",
      "Estimated cost for feature 150 is  27687.7989611\n",
      "Estimated cost for feature 151 is  28645.6726424\n",
      "Estimated cost for feature 152 is  29086.3274437\n",
      "Estimated cost for feature 153 is  29369.9084581\n",
      "Estimated cost for feature 154 is  38461.1331558\n",
      "Estimated cost for feature 155 is  40284.0025441\n",
      "Estimated cost for feature 156 is  41778.0876213\n",
      "Estimated cost for feature 157 is  14878.9476486\n",
      "Estimated cost for feature 158 is  25494.85119\n",
      "Estimated cost for feature 159 is  26507.0766086\n",
      "Estimated cost for feature 160 is  26366.6785618\n",
      "Estimated cost for feature 161 is  29753.5582961\n",
      "Estimated cost for feature 162 is  29778.2150033\n",
      "Estimated cost for feature 163 is  27350.1855548\n",
      "Estimated cost for feature 164 is  37826.0271597\n",
      "Estimated cost for feature 165 is  40575.1313116\n",
      "Estimated cost for feature 166 is  42141.145196\n",
      "Estimated cost for feature 167 is  8723.9769029\n",
      "Estimated cost for feature 168 is  8764.80771744\n",
      "Estimated cost for feature 169 is  8724.5074031\n",
      "Estimated cost for feature 170 is  11484.1694562\n",
      "Estimated cost for feature 171 is  9883.7249461\n",
      "Estimated cost for feature 172 is  10403.9435382\n",
      "Estimated cost for feature 173 is  13929.0359743\n",
      "Estimated cost for feature 174 is  15725.4170161\n",
      "Estimated cost for feature 175 is  15432.83423\n",
      "Estimated cost for feature 176 is  21843.3779451\n",
      "Estimated cost for feature 177 is  19866.3915424\n",
      "Estimated cost for feature 178 is  23372.6014669\n",
      "Estimated cost for feature 179 is  21025.682043\n",
      "Estimated cost for feature 180 is  23562.5690667\n",
      "Estimated cost for feature 181 is  24041.0262871\n",
      "Estimated cost for feature 182 is  24309.3487509\n",
      "Estimated cost for feature 183 is  25442.213717\n",
      "Estimated cost for feature 184 is  23477.0299855\n",
      "Estimated cost for feature 185 is  24511.5287193\n",
      "Estimated cost for feature 186 is  24548.2954933\n",
      "Estimated cost for feature 187 is  24632.8041691\n",
      "Estimated cost for feature 188 is  26320.1334559\n",
      "Estimated cost for feature 189 is  26766.020195\n",
      "Estimated cost for feature 190 is  28447.681671\n",
      "Estimated cost for feature 191 is  24337.1167353\n",
      "Estimated cost for feature 192 is  25049.1287293\n",
      "Estimated cost for feature 193 is  23026.5186789\n",
      "Estimated cost for feature 194 is  25033.4025893\n",
      "Estimated cost for feature 195 is  26906.4758026\n",
      "Estimated cost for feature 196 is  27745.6779171\n",
      "Estimated cost for feature 197 is  27300.1926018\n",
      "Estimated cost for feature 198 is  26485.427137\n",
      "Estimated cost for feature 199 is  29248.1635389\n",
      "Estimated cost for feature 200 is  27993.5845154\n",
      "Estimated cost for feature 201 is  25906.7282291\n",
      "Estimated cost for feature 202 is  23433.4137146\n",
      "Estimated cost for feature 203 is  28011.9188175\n",
      "Estimated cost for feature 204 is  29140.5866059\n",
      "Estimated cost for feature 205 is  27930.5618505\n",
      "Estimated cost for feature 206 is  26638.3763498\n",
      "Estimated cost for feature 207 is  27815.9057038\n",
      "Estimated cost for feature 208 is  30353.9995262\n",
      "Estimated cost for feature 209 is  36761.0783373\n",
      "Estimated cost for feature 210 is  40904.0479545\n",
      "Estimated cost for feature 211 is  42018.9669419\n",
      "Estimated cost for feature 212 is  9230.96405567\n",
      "Estimated cost for feature 213 is  9403.85734224\n",
      "Estimated cost for feature 214 is  9797.93013363\n",
      "Estimated cost for feature 215 is  11850.4707666\n",
      "Estimated cost for feature 216 is  10594.7611596\n",
      "Estimated cost for feature 217 is  11651.0697183\n",
      "Estimated cost for feature 218 is  15101.0964621\n",
      "Estimated cost for feature 219 is  16918.9907979\n",
      "Estimated cost for feature 220 is  14752.2438821\n",
      "Estimated cost for feature 221 is  21332.6548695\n",
      "Estimated cost for feature 222 is  20871.2417075\n",
      "Estimated cost for feature 223 is  24332.1060017\n",
      "Estimated cost for feature 224 is  21527.0941003\n",
      "Estimated cost for feature 225 is  25471.1740318\n",
      "Estimated cost for feature 226 is  25395.0608929\n",
      "Estimated cost for feature 227 is  24626.8387673\n",
      "Estimated cost for feature 228 is  26033.5961819\n",
      "Estimated cost for feature 229 is  23874.6272419\n",
      "Estimated cost for feature 230 is  24841.4854374\n",
      "Estimated cost for feature 231 is  25766.3890909\n",
      "Estimated cost for feature 232 is  23774.9582635\n",
      "Estimated cost for feature 233 is  26256.9819702\n",
      "Estimated cost for feature 234 is  27566.5044112\n",
      "Estimated cost for feature 235 is  29339.438444\n",
      "Estimated cost for feature 236 is  24819.6779369\n",
      "Estimated cost for feature 237 is  25571.0271698\n",
      "Estimated cost for feature 238 is  24441.3979169\n",
      "Estimated cost for feature 239 is  26034.6307811\n",
      "Estimated cost for feature 240 is  28412.8302881\n",
      "Estimated cost for feature 241 is  28992.0516943\n",
      "Estimated cost for feature 242 is  27938.305569\n",
      "Estimated cost for feature 243 is  27506.7314796\n",
      "Estimated cost for feature 244 is  29791.938015\n",
      "Estimated cost for feature 245 is  29585.3750974\n",
      "Estimated cost for feature 246 is  28984.5220978\n",
      "Estimated cost for feature 247 is  25015.6075201\n",
      "Estimated cost for feature 248 is  28960.5373952\n",
      "Estimated cost for feature 249 is  30057.5705771\n",
      "Estimated cost for feature 250 is  29120.0355289\n",
      "Estimated cost for feature 251 is  29751.5360103\n",
      "Estimated cost for feature 252 is  29493.3176082\n",
      "Estimated cost for feature 253 is  31650.4983827\n",
      "Estimated cost for feature 254 is  40427.5190868\n",
      "Estimated cost for feature 255 is  41590.18947\n",
      "Estimated cost for feature 256 is  43019.2937223\n",
      "Estimated cost for feature 257 is  7366.13423433\n",
      "Estimated cost for feature 258 is  6897.04737756\n",
      "Estimated cost for feature 259 is  8335.3117843\n",
      "Estimated cost for feature 260 is  7336.94628792\n",
      "Estimated cost for feature 261 is  8662.2214385\n",
      "Estimated cost for feature 262 is  8618.23295005\n",
      "Estimated cost for feature 263 is  9160.13663315\n",
      "Estimated cost for feature 264 is  9543.23606441\n",
      "Estimated cost for feature 265 is  7671.90580192\n",
      "Estimated cost for feature 266 is  8821.47302073\n",
      "Estimated cost for feature 267 is  8005.1817651\n",
      "Estimated cost for feature 268 is  8260.81676911\n",
      "Estimated cost for feature 269 is  8937.6225929\n",
      "Estimated cost for feature 270 is  9393.27850386\n",
      "Estimated cost for feature 271 is  9605.26283113\n",
      "Estimated cost for feature 272 is  8953.31481751\n",
      "Estimated cost for feature 273 is  7985.19569938\n",
      "Estimated cost for feature 274 is  8579.3495672\n",
      "Estimated cost for feature 275 is  9204.13635897\n",
      "Estimated cost for feature 276 is  9024.12288527\n",
      "Estimated cost for feature 277 is  9426.95033265\n",
      "Estimated cost for feature 278 is  9898.76279468\n",
      "Estimated cost for feature 279 is  10458.2067541\n",
      "Estimated cost for feature 280 is  11457.7702717\n",
      "Estimated cost for feature 281 is  11475.5469303\n",
      "Estimated cost for feature 282 is  10886.5926217\n",
      "Estimated cost for feature 283 is  8941.48701138\n",
      "Estimated cost for feature 284 is  10231.3747175\n",
      "Estimated cost for feature 285 is  10706.6991232\n",
      "Estimated cost for feature 286 is  10103.5055714\n",
      "Estimated cost for feature 287 is  10755.553241\n",
      "Estimated cost for feature 288 is  10989.8517058\n",
      "Estimated cost for feature 289 is  11315.1049134\n",
      "Estimated cost for feature 290 is  14146.9700878\n",
      "Estimated cost for feature 291 is  13834.8138805\n",
      "Estimated cost for feature 292 is  17596.3974781\n",
      "Estimated cost for feature 293 is  20264.5212056\n",
      "Estimated cost for feature 294 is  20512.8929856\n",
      "Estimated cost for feature 295 is  19893.1310024\n",
      "Estimated cost for feature 296 is  21322.383928\n",
      "Estimated cost for feature 297 is  21193.7159854\n",
      "Estimated cost for feature 298 is  21665.7157029\n",
      "Estimated cost for feature 299 is  21081.2426982\n",
      "Estimated cost for feature 300 is  20367.5847745\n",
      "Estimated cost for feature 301 is  19143.6699283\n",
      "Estimated cost for feature 302 is  19611.2670822\n",
      "Estimated cost for feature 303 is  20403.4240504\n",
      "Estimated cost for feature 304 is  19461.7644947\n",
      "Estimated cost for feature 305 is  20724.228822\n",
      "Estimated cost for feature 306 is  19118.1673023\n",
      "Estimated cost for feature 307 is  23609.8948962\n",
      "Estimated cost for feature 308 is  23545.7409896\n",
      "Estimated cost for feature 309 is  23342.2096127\n",
      "Estimated cost for feature 310 is  22929.7008618\n",
      "Estimated cost for feature 311 is  21644.9969627\n",
      "Estimated cost for feature 312 is  21654.5380064\n",
      "Estimated cost for feature 313 is  22340.7068992\n",
      "Estimated cost for feature 314 is  23018.8555768\n",
      "Estimated cost for feature 315 is  24318.698704\n",
      "Estimated cost for feature 316 is  24407.4958357\n",
      "Estimated cost for feature 317 is  25519.3307842\n",
      "Estimated cost for feature 318 is  23718.3989774\n",
      "Estimated cost for feature 319 is  26451.7338307\n",
      "Estimated cost for feature 320 is  26416.8578068\n",
      "Estimated cost for feature 321 is  23487.9834118\n",
      "Estimated cost for feature 322 is  23263.847203\n",
      "Estimated cost for feature 323 is  20827.4522022\n",
      "Estimated cost for feature 324 is  23856.7541612\n",
      "Estimated cost for feature 325 is  24773.6971928\n",
      "Estimated cost for feature 326 is  22982.8380342\n",
      "Estimated cost for feature 327 is  23619.2714852\n",
      "Estimated cost for feature 328 is  23728.0029343\n",
      "Estimated cost for feature 329 is  26116.8403429\n",
      "Estimated cost for feature 330 is  24409.3627877\n",
      "Estimated cost for feature 331 is  23171.776954\n",
      "Estimated cost for feature 332 is  22945.2785186\n",
      "Estimated cost for feature 333 is  24415.8140337\n",
      "Estimated cost for feature 334 is  24514.9770929\n",
      "Estimated cost for feature 335 is  23396.1445359\n",
      "Estimated cost for feature 336 is  23678.7931647\n",
      "Estimated cost for feature 337 is  24212.0315624\n",
      "Estimated cost for feature 338 is  24466.2464298\n",
      "Estimated cost for feature 339 is  25797.1825419\n",
      "Estimated cost for feature 340 is  27777.5329858\n",
      "Estimated cost for feature 341 is  29070.6775664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-51c96a1511e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_set_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, samples, outcome_variable)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtraining_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcome_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mpredicting_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_decision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicting_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_decision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36mbuild_decision_tree\u001b[0;34m(self, samples, features)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mbest_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_best_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"BEST FEATURE THIS ITERATION IS \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_feature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'stop'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36mselect_best_feature\u001b[0;34m(self, samples, features, classes)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m    118\u001b[0m         gain_factors = [(self.score_function(samples, feat, classes, features), feat)\n\u001b[0;32m--> 119\u001b[0;31m                         for feat in features]\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mgain_factors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"GAIN FACTORS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain_factors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36mscore_function\u001b[0;34m(self, samples, feature, classes, features_left)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0minfo_gain1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_gain2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m#print \"         Info gain is\",info_gain1-info_gain2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mestimated_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimated_Cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mestimated_cost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36mEstimated_Cost\u001b[0;34m(self, samples, feature, classes, features_left)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mPL\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mfirst_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPL\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0msecond_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mPL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP_L_conditional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_left\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#need_features_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;31m#print \"2nd\",second_part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_part\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msecond_part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36mP_L_conditional\u001b[0;34m(self, samples, feature, classes, features_left)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypothetical_featuresleft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP_L\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0msums\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_conditional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36mP_L\u001b[0;34m(self, samples, feature, classes)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mP_L\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mparent_entrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_entrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchild_entrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_entrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36minformation_gain\u001b[0;34m(self, samples, feature, classes)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpartition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples_partition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0msub_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mfeature_entropy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;31m#print \"Child is\", feature_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-059faef8ab55>\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;31m#rint counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/courtneycochrane/anaconda/envs/py27/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/courtneycochrane/anaconda/envs/py27/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mself_get\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = DecisionTree()\n",
    "d.fit(training_set_X,training_set_Y)\n",
    "d.print_tree()\n",
    "\n",
    "y_pred = d.predict(test_set_X)\n",
    "\n",
    "count = []\n",
    "recall = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == test_set_Y[i]:\n",
    "        count.append(1)\n",
    "    else:\n",
    "        count.append(0)\n",
    "        \n",
    "    if test_set_Y[i] == 1:\n",
    "        if y_pred[i] == 1:\n",
    "            recall.append(1)\n",
    "        else:\n",
    "            recall.append(0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
