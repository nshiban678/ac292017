{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  =============================================================================================================================\n",
    "#                                         SCRIPT FOR FEATURE REDUCTION or FEATURE SELECTION\n",
    "#  lAST UPDATE: 03/19/2017\n",
    "#  ============================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "#                                            STEP 1: Import libraries\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "#%reset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "import matplotlib as mpl\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sys import stdout\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ismember(x, y):\n",
    "    \"\"\"Function to find indexes of a list (x) within another list (y) in the same order as indicate by x\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    -x:list\n",
    "    -y list\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    indexes in list \"y\" where the elements of \"x\" are located.\n",
    "    \"\"\"\n",
    "        \n",
    "    bind = {}\n",
    "    for i, elt in enumerate(y):\n",
    "        if elt not in bind:\n",
    "            bind[elt] = i\n",
    "    return [bind.get(itm, None) for itm in x]  # None can be replaced by any other \"not in b\" value\n",
    "\n",
    "def load_numerical_table(fileName):\n",
    "    \"\"\"Function to load .csv file that has numerical-only data; it also displays some basic info about the .csv file content\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    -fileName: path of rthe .csv file we want to load; put file name only if file is located in current path\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    InputMatrix  = numpy array with all values, equivalent to the values in the input .csv file (but no headers)\n",
    "    InputHeaders = list with headers in input .csv file\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Load .csv file into panda dataframe and show head\n",
    "    # ----------------------------------------------------\n",
    "    transformed_data = pd.read_csv(fileName)\n",
    "    display(transformed_data.head(3))\n",
    "\n",
    "    # Put headers into list\n",
    "    # ----------------------\n",
    "    InputHeaders=list(transformed_data)\n",
    "\n",
    "\n",
    "    # Turn data frames into matrix\n",
    "    # ---------------------------------\n",
    "    InputMatrix  = transformed_data.as_matrix(); InputMatrix = np.array(InputMatrix) ;\n",
    "    #print \"Input Matrix Size:\"; print \"-----------------------\"\n",
    "    #print InputMatrix.shape\n",
    "\n",
    "    # Turn -1000 into NaNs\n",
    "    # ---------------------------------\n",
    "    #InputMatrix[InputMatrix==-1000] = np.nan\n",
    "    \n",
    "    return InputMatrix,InputHeaders\n",
    "\n",
    "def NormalizeData(InputMatrix_train,InputMatrix_test,NormalizationMethod):\n",
    "    \"\"\"Function to normalize all columns in a matrix according to NormalizationMethod (see below)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    -InputMatrix: Matrix we want to normalize\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    NormalizationMethod: 'MeanStd' or 'MinMax' for now.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # OPTION 1: Mean and Std\n",
    "    # -----------------------\n",
    "    # Transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features \n",
    "    # by their standard deviation.Scaled data wil have zero mean and unit variance\n",
    "    if NormalizationMethod=='MeanStd':\n",
    "        #X= preprocessing.scale(InputMatrix_train)\n",
    "        normMap=preprocessing.StandardScaler().fit(InputMatrix_train)\n",
    "        X_train=normMap.transform(InputMatrix_train) \n",
    "        X_test=normMap.transform(InputMatrix_test) \n",
    "        \n",
    "\n",
    "    # OPTION 2: Min and Max\n",
    "    # ----------------------\n",
    "    # Scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute \n",
    "    # value of each feature is scaled to unit size.\n",
    "    if NormalizationMethod=='MinMax':\n",
    "        \n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        normMap=min_max_scaler.fit(InputMatrix_train)\n",
    "        X_train = min_max_scaler.fit_transform(InputMatrix_train,normMap)\n",
    "        X_test = min_max_scaler.fit_transform(InputMatrix_test,normMap)      \n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def featureSelection(X,XHeaders,X_test,XHeaders_test,FeatureReductionMethod,ExplainedVariance,APpreference):\n",
    "    \n",
    "    # METHOD 1: SVD (Singular Value Decomposition)\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    if FeatureReductionMethod=='SVD':\n",
    "\n",
    "        # --------------------------------------\n",
    "        # OPTION 1: Singular Vale Decomposition\n",
    "        # --------------------------------------\n",
    "\n",
    "        # Linear dimensionality reduction using Singular Value Decomposition of centered data,  keeping only the most significant \n",
    "        # singular vectors to project the data to a lower dimensional space.\n",
    "\n",
    "        # References:\n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html\n",
    "\n",
    "        print \" Feature reduction implemented using SVD \"\n",
    "        print \"----------------------------------------------------------------------------\"\n",
    "\n",
    "        \n",
    "        # Define PCA\n",
    "        pca = PCA(n_components=ExplainedVariance)\n",
    "\n",
    "        # Compute PCA\n",
    "        pca.fit(X)\n",
    "\n",
    "        # Apply dimensionality reduction to X_train and X_test\n",
    "        Xreduced=pca.transform(X);\n",
    "        Xreduced_test=pca.transform(X_test);\n",
    "        \n",
    "\n",
    "        # Explain variance by each component\n",
    "        print \"- Original train matrix has size \" + str(X.shape)\n",
    "        print \"- Reduced train matrix has size \" + str(Xreduced.shape)\n",
    "        print \"- Original test matrix has size \" + str(X_test.shape)\n",
    "        print \"- Reduced test matrix has size \" + str(Xreduced_test.shape)\n",
    "        print \"- Number of components needed to explain \" + str(ExplainedVariance*100)+ \"(%) is\", pca.explained_variance_ratio_.size\n",
    "        \n",
    "        # New Feature Headers\n",
    "        pcrange=range(1,pca.explained_variance_ratio_.size+1)  \n",
    "        myheaders = map(str,pcrange)\n",
    "        myheaders = [\"PC\" + myheaders for myheaders in myheaders]\n",
    "        HeadersReduced=','.join(myheaders)\n",
    "        HeadersReduced_test=','.join(myheaders)\n",
    "        \n",
    "    if FeatureReductionMethod=='AffinityPropagation':\n",
    "\n",
    "        # --------------------------------------\n",
    "        # OPTION 2: Affinity Propagation\n",
    "        # --------------------------------------\n",
    "\n",
    "        # Affinity Propagation is a clustering technique that identifies \"exemplars\" (which will be our reduced features)\n",
    "\n",
    "        # References:\n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation.fit\n",
    "      \n",
    "\n",
    "        print \" Feature reduction implementing using Affinity Propagation \"\n",
    "        print \"----------------------------------------------------------------------------\"\n",
    "        \n",
    "        # Apply Affinity Propagation to matrix X.T\n",
    "        af = AffinityPropagation(preference=APpreference).fit(X.T)\n",
    "        \n",
    "        \n",
    "        # Identify indices for examplars (i.e., features that are cluster centers)\n",
    "        cluster_centers_indices = af.cluster_centers_indices_\n",
    "        \n",
    "        # Reduce train set\n",
    "        HeadersReduced = [XHeaders[i] for i in cluster_centers_indices]\n",
    "        Xreduced=X[:,cluster_centers_indices]\n",
    "        \n",
    "        # Reduce test set\n",
    "        Xreduced_test=X_test[:,cluster_centers_indices]\n",
    "        HeadersReduced_test=HeadersReduced\n",
    "        \n",
    "        #print \"Selected features:\"\n",
    "        #print cluster_centers_indices\n",
    "        #print HeadersReduced\n",
    "\n",
    "        # Identify labels for each feature (i.e., to which cluster center they are linked)\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        labels = af.labels_\n",
    "        #print labels\n",
    "        #print 'Affinity Propagation finished'\n",
    "        print \"- Original train matrix has size \" + str(X.shape)\n",
    "        print \"- Reduced train matrix has size \" + str(Xreduced.shape)\n",
    "        print \"- Original test matrix has size \" + str(X_test.shape)\n",
    "        print \"- Reduced test matrix has size \" + str(Xreduced_test.shape)\n",
    "        print \"- Number of features selected \" + str(len(HeadersReduced))\n",
    "\n",
    "    if FeatureReductionMethod=='none':\n",
    "        Xreduced = X[:,:]\n",
    "        HeadersReduced = XHeaders\n",
    "        Xreduced_test = X_test[:,:]\n",
    "        HeadersReduced_test = HeadersReduced\n",
    "        \n",
    "    return Xreduced,HeadersReduced,Xreduced_test,HeadersReduced_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RunFeatureReduction(InputToFeatureReduction_train,OutputFromFeatureReduction_train,InputToFeatureReduction_test,\\\n",
    "                        OutputFromFeatureReduction_test,NormalizationMethod,FeatureReductionMethod,\\\n",
    "                       ExplainedVariance,APpreference):\n",
    "    \"\"\"Main function to run Feature Reduction\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - InputToInputToFeatureReduction: Input file (.csv)\n",
    "    - OutputFromFeatureReduction: Output file (.csv)\n",
    "    - NormalizationMethod: 'MinMax' or 'MeanStd'\n",
    "    - FeatureReductionMethod: 'SVD'\n",
    "    - ExplainedVariance: This applies to SVD, and it specifies the amount of variance we want to explain from input space.\n",
    "    -\n",
    "    Returns:\n",
    "    --------\n",
    "    Output is a .csv file with name <OutputFromFeatureReduction>\n",
    "    \"\"\"\n",
    "        \n",
    "    # Load train and test data from .csv numerical-only table and do some basic arrangements\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    InputMatrix_train,InputHeaders_train=load_numerical_table(InputToFeatureReduction_train)\n",
    "    InputMatrix_test,InputHeaders_test=load_numerical_table(InputToFeatureReduction_test)\n",
    "\n",
    "\n",
    "    # Feature space normalization on training data\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    X_train,X_test=NormalizeData(InputMatrix_train,InputMatrix_test,NormalizationMethod)\n",
    "   \n",
    "\n",
    "    # Feature reduction for training set\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    Xinput_train=X_train[:,:-1];\n",
    "    XHeaders_train=InputHeaders_train[:-1]\n",
    "    Xinput_test=X_test[:,:-1];\n",
    "    XHeaders_test=InputHeaders_test[:-1]\n",
    "    Xreduced_train,HeadersReduced_train,Xreduced_test,HeadersReduced_test=featureSelection(Xinput_train,XHeaders_train,Xinput_test,XHeaders_test,\\\n",
    "                                                                                           FeatureReductionMethod,ExplainedVariance,APpreference)\n",
    "\n",
    "\n",
    "    # Report and Export .csv file for train set\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "    NewOutputMatrix_train=InputMatrix_train[:,-1];\n",
    "    NewOutputHeader_train=InputHeaders_train[-1];\n",
    "    # Save dataframe to human readable data\n",
    "    myMatrix_train = np.append(Xreduced_train,  NewOutputMatrix_train[:, None], 1);\n",
    "    myHeader_train=  \",\".join((str(HeadersReduced_train), NewOutputHeader_train))\n",
    "    np.savetxt(OutputFromFeatureReduction_train,myMatrix_train,header=myHeader_train,delimiter=',')\n",
    "    \n",
    "    # Report and Export .csv file for test set\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "    NewOutputMatrix_test=InputMatrix_test[:,-1];\n",
    "    NewOutputHeader_test=InputHeaders_test[-1];\n",
    "    # Save dataframe to human readable data\n",
    "    myMatrix_test = np.append(Xreduced_test,  NewOutputMatrix_test[:, None], 1);\n",
    "    myHeader_test=  \",\".join((str(HeadersReduced_test), NewOutputHeader_test))\n",
    "    np.savetxt(OutputFromFeatureReduction_test,myMatrix_test,header=myHeader_test,delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
