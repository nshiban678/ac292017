{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: n x 3 csv matrix, results.csv, with 1st column = the predicted label and the 2nd column = the actual label, 3rd column the probabilities\n",
    "\n",
    "Output: Model performance metrics (graphs of ROC curves, confusion matrices, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_performance(performance_metric):\n",
    "    \"\"\"Function that returns given model performance metric\n",
    "    \n",
    "    Parameter Options:\n",
    "    -----------------\n",
    "    performance metrix options: 'confusion_matrix', 'recall', 'precision',\n",
    "    'accuracy', 'f1', 'roc'\n",
    "    \n",
    "    if 'all' specified returns list of [accuracy, recall, precision, f1]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.loadtxt('results.csv', delimiter=\",\") #read in data \n",
    "    predicted = data[:,0]\n",
    "    true = data[:,1]\n",
    "    \n",
    "    prec, rec, f1, sup = precision_recall_fscore_support(true, predicted, average= 'binary')\n",
    "    acc = accuracy_score(true, predicted)\n",
    "    \n",
    "    if performance_metric == 'confusion_matrix':\n",
    "        #Count of true negatives is C_{0,0}, false negatives is C_{1,0},\n",
    "        #true positives is C_{1,1} and false positives is C_{0,1}\n",
    "        return confusion_matrix(data[:,1], data[:,0], labels = [0,1])\n",
    "    \n",
    "    if performance_metric == \"recall\":\n",
    "        return rec\n",
    "    \n",
    "    if performance_metric == \"accuracy\":\n",
    "        return acc\n",
    "\n",
    "    if performance_metric == \"f1\":\n",
    "        return f1\n",
    "    \n",
    "    if performance_metric == \"precision\":\n",
    "        return prec\n",
    "    \n",
    "    if performance_metric == 'roc':\n",
    "        probabilities = data[:,2]\n",
    "        fpr, tpr, _ = roc_curve(true, probabilities)\n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1])\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC curve')\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        return [acc,rec,prec,f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
