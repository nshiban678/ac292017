{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for importing iPython notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.worksheets[0].cells:\n",
    "                if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import pandas\n",
    "import csv\n",
    "import MergeDiagnosis_AdjustedLabels\n",
    "import Categorical_Updated\n",
    "import LongitudinalDataAnalysis\n",
    "import Imputation\n",
    "import FeatureReduction\n",
    "import SupervisedLearning\n",
    "import ModelPerformance\n",
    "import TrainTestSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Merge Data\n",
    "# -------------------------------\n",
    "merged_data = MergeDiagnosis_AdjustedLabels.data_preprocess(study = \"all\",imaging_to_drop = 'all', reversions = 'label0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Categorical to Numerical\n",
    "# -------------------------------\n",
    "date_cols = ['update_stamp','EXAMDATE','EXAMDATE_bl']\n",
    "cols_to_ignore = ['PTID']\n",
    "\n",
    "Categorical_Updated.categorical_conversion(date_cols,cols_to_ignore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Longitudinal Data Analysis\n",
    "# -------------------------------\n",
    "\n",
    "# Input file name for Longitudinal Data Analysis\n",
    "InputToLongitudinal='CategoricalToNumerical.csv'\n",
    "\n",
    "with open(InputToLongitudinal) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    labels = next(reader)\n",
    "\n",
    "# Output file name from this script\n",
    "OutputFromLongitudinal='LongitudinalDataAnalysis.csv'\n",
    "\n",
    "# Patient RID Features\n",
    "Patient_FEATURES=['RID'];\n",
    "\n",
    "# Demographic Features\n",
    "Demo_FEATURES_type=['AGE','PTEDUCAT','PTGENDER','PTETHCAT','PTRACCAT','PTMARRY']\n",
    "Demo_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "\n",
    "# Baseline OneTime Features\n",
    "BaselineOneTime_FEATURES_type = ['APOE4','Years_bl','ORIGPROT']\n",
    "BaselineOneTime_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "    elif i.rfind(\"_\") != -1 and i[:i.rfind(\"_\")] in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "\n",
    "# Time Headers\n",
    "Time_FEATURES_type=['SITE','Month','update_stamp_minus_EXAMDATE_bl','update_stamp_minus_EXAMDATE','EXAMDATE_minus_EXAMDATE_bl',\n",
    "               'COLPROT','M','Month_bl']\n",
    "Time_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "Time_FEATURES.insert(len(Time_FEATURES), Time_FEATURES.pop(Time_FEATURES.index('Month_bl'))) # Month_bl must be the last time feature\n",
    "\n",
    "# Baseline Evaluation Features\n",
    "BaselineEvaluation_FEATURES=['CDRSB_bl','ADAS11_bl','ADAS13_bl','MMSE_bl','RAVLT_learning_bl','RAVLT_forgetting_bl',\n",
    "                             'RAVLT_perc_forgetting_bl','RAVLT_immediate_bl','FAQ_bl','MOCA_bl','EcogPtLang_bl','EcogPtVisspat_bl',\n",
    "                             'EcogPtPlan_bl','EcogPtOrgan_bl','EcogPtDivatt_bl','EcogPtMem_bl','EcogPtTotal_bl','EcogSPLang_bl',\n",
    "                             'EcogSPVisspat_bl','EcogSPPlan_bl','EcogSPOrgan_bl','EcogSPDivatt_bl','EcogSPMem_bl','EcogSPTotal_bl'];\n",
    "\n",
    "\n",
    "\n",
    "# Current Medical Evaluation\n",
    "CurrentEvaluation_FEATURES=['CDRSB','ADAS11','ADAS13','MMSE','RAVLT_learning','RAVLT_forgetting','RAVLT_perc_forgetting','RAVLT_immediate',\n",
    "                            'FAQ','MOCA','EcogPtLang','EcogPtVisspat','EcogPtPlan','EcogPtOrgan','EcogPtDivatt','EcogPtMem','EcogPtTotal',\n",
    "                            'EcogSPLang','EcogSPVisspat','EcogSPPlan','EcogSPOrgan','EcogSPDivatt','EcogSPMem','EcogSPTotal'];\n",
    "\n",
    "\n",
    "# Current Diagnosis\n",
    "CurrentDiagnosis_FEATURES= ['AD'];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    print \"###########################################\"; \n",
    "    print \"###########################################\";\n",
    "    print i;\n",
    "    print \"###########################################\";\n",
    "    print \"###########################################\";\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Longitudinal Data Analysis\n",
    "    # -------------------------------\n",
    "\n",
    "    # Longitudinal Method\n",
    "    LongitudinalMethod=2;\n",
    "    MetricList=['MaxTime','Delta','Mean','Std'];\n",
    "\n",
    "    # Run Longitudinal Data Anaysis\n",
    "    LongitudinalDataAnalysis.runLongitudinal(InputToLongitudinal,OutputFromLongitudinal,Patient_FEATURES,Demo_FEATURES,\\\n",
    "                                             BaselineOneTime_FEATURES,Time_FEATURES,BaselineEvaluation_FEATURES,\\\n",
    "                                             CurrentEvaluation_FEATURES,CurrentDiagnosis_FEATURES,LongitudinalMethod,MetricList)\n",
    "\n",
    "    # # !!!!!!!!!!!!!!!!!!!!!!--------------------------------------------------------------\n",
    "    # Keep only a few columns \n",
    "    # --------------------------------------------------------------\n",
    "    import pandas as pd\n",
    "    f=pd.read_csv(\"LongitudinalDataAnalysis.csv\")\n",
    "    keep_col = ['CDRSB_MaxTime','CDRSB_Delta','CDRSB_Mean','CDRSB_Std',\\\n",
    "                'ADAS13_MaxTime','ADAS13_Delta','ADAS13_Mean','ADAS13_Std',\\\n",
    "                'MOCA_MaxTime','MOCA_Delta','MOCA_Mean','MOCA_Std',\\\n",
    "                'Diagnostics']\n",
    "    new_f = f[keep_col]\n",
    "    new_f.to_csv(\"LongitudinalDataAnalysis.csv\", index=False)\n",
    "    \n",
    "    \n",
    "    # ------------------\n",
    "    # Train Test Split\n",
    "    # ------------------\n",
    "    TrainTestSplit.traintest_split(0.33)\n",
    "\n",
    "\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # Limit number of months available for test patients\n",
    "    # ----------------------------------------------------\n",
    "    # Longitudinal Method\n",
    "    LongitudinalMethod=3;\n",
    "\n",
    "    # Run Longitudinal Data Anaysis\n",
    "    LongitudinalDataAnalysis.runLongitudinal(InputToLongitudinal,OutputFromLongitudinal,Patient_FEATURES,Demo_FEATURES,\\\n",
    "                                             BaselineOneTime_FEATURES,Time_FEATURES,BaselineEvaluation_FEATURES,\\\n",
    "                                             CurrentEvaluation_FEATURES,CurrentDiagnosis_FEATURES,LongitudinalMethod,MetricList)\n",
    "\n",
    "         \n",
    "    # !!!!!!!!!!!!!!!!!!!!!! --------------------------------------------------------------\n",
    "    # Keep only a few columns \n",
    "    # --------------------------------------------------------------\n",
    "    f=pd.read_csv(\"LongitudinalDataAnalysis.csv\")\n",
    "    new_f = f[keep_col]\n",
    "    new_f.to_csv(\"LongitudinalDataAnalysis.csv\", index=False)\n",
    "    \n",
    "    f2=pd.read_csv(\"LongitudinalDataAnalysis_test.csv\")\n",
    "    new_f2 = f2[keep_col]\n",
    "    new_f2.to_csv(\"LongitudinalDataAnalysis_test.csv\", index=False)\n",
    "    \n",
    "    \n",
    "    # ------------------\n",
    "    # Imputation\n",
    "    # ------------------\n",
    "    #Imputation.imputation('knn')\n",
    "    Imputation.imputation('meanmode')\n",
    "    #Imputation.imputation('nuclearnorm')\n",
    "    #Imputation.imputation('softimpute')\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Feature Reduction \n",
    "    # ----------------------------------\n",
    "\n",
    "    # Input and Output files from Feature Reduction for train set\n",
    "    InputToFeatureReduction_train     ='ImputedMatrix_train.csv'\n",
    "    OutputFromFeatureReduction_train  ='Features_train.csv'\n",
    "    InputToFeatureReduction_test      ='ImputedMatrix_test.csv'\n",
    "    OutputFromFeatureReduction_test   ='Features_test.csv'\n",
    "\n",
    "\n",
    "    # Normalization method\n",
    "    NormalizationMethod='MinMax'\n",
    "    #NormalizationMethod='MeanStd'\n",
    "\n",
    "    # Feature Reduction Method and Settings\n",
    "    #FeatureReductionMethod='SVD'; \n",
    "    ExplainedVariance=0.99; # For method 'SVD'\n",
    "\n",
    "    FeatureReductionMethod='none' # 'AffinityPropagation', 'SVD' or 'none' \n",
    "    APpreference=-50; # Hyperparameter for method 'AffinityPropagation'\n",
    "\n",
    "\n",
    "    # Run Feature Reduction\n",
    "    FeatureReduction.RunFeatureReduction(InputToFeatureReduction_train,OutputFromFeatureReduction_train,\\\n",
    "                                         InputToFeatureReduction_test,OutputFromFeatureReduction_test,\\\n",
    "                                         NormalizationMethod,FeatureReductionMethod,ExplainedVariance,APpreference)\n",
    "\n",
    "\n",
    "    # ------------------\n",
    "    # Supervised Learning- Model 3\n",
    "    # ------------------\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import numpy as np\n",
    "    data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "    data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "    training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "    best_params = {'n_estimators': 1000}\n",
    "    model = RandomForestClassifier()\n",
    "    SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "    res.append(ModelPerformance.model_performance('all'))\n",
    "\n",
    "    #------------------\n",
    "    # Model Evaluation\n",
    "    # ------------------\n",
    "    #print ModelPerformance.model_performance('confusion_matrix')\n",
    "    print ModelPerformance.model_performance('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print res\n",
    "res2=np.asarray(res)\n",
    "print res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(res2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # ------------------\n",
    "# # Supervised Learning- options currently are LogisticRegression, RandomForest, MLP, knn\n",
    "# # ------------------\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import numpy as np\n",
    "# data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "# data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "\n",
    "# training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "\n",
    "# best_params = {'alpha': 0.05, 'hidden_layer_sizes': (100,100,100)}\n",
    "# model = MLPClassifier()\n",
    "# SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "\n",
    "# #------------------\n",
    "# # Model Evaluation\n",
    "# # ------------------\n",
    "# #print ModelPerformance.model_performance('confusion_matrix')\n",
    "# print ModelPerformance.model_performance('all')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
