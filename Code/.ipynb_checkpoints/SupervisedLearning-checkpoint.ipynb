{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Features.csv file that contains output variable in last column  \n",
    "\n",
    "Output: n x 2 csv matrix (results.csv) with 1st column = the predicted label and the 2nd column = the actual label \n",
    "\n",
    "Driver function below called TuneAndReport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning Code with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneHyperparameters(classifier, training_set_X, training_set_Y, hyperparameters, number_of_folds, type_of_score):\n",
    "    \"\"\"This function runs k-fold cross validation on given set of parameters and returns best combo of parameters (according to specified model performance metric)\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    -classifier: base classifer, parameters will be set during this function\n",
    "    -training_set_X: X data\n",
    "    -training_set_Y: Y data\n",
    "    -hyperparameters: dictionary of parameter name to options\n",
    "    -number_of_folds: number of k-fold folds\n",
    "    -type_of_score: performance metric used to compare hyperparameters, options are \"recall\", \"precision\", \"f1\", or \"accuracy\"\n",
    "    \n",
    "    Returns: \n",
    "    -------\n",
    "    Dictionary of best parameter values, for example {'C': 2, 'penalty': 'l2'}\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    parameters = {'C': [3,2,5,6], 'penalty': ['l2']}\n",
    "    tuneHyperparameters(LogisticRegression(), X, Y, parameters, 5, 'accuracy')\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits= number_of_folds)\n",
    "    kf.get_n_splits(training_set_X)\n",
    "    \n",
    "    allNames = sorted(hyperparameters)\n",
    "    parameter_combos = it.product(*(hyperparameters[Name] for Name in allNames))\n",
    "    \n",
    "    metrics = []\n",
    "    params = []\n",
    "    for hyperparameter_combo in parameter_combos:\n",
    "        params.append(hyperparameter_combo)\n",
    "        for p in range(len(allNames)):\n",
    "            classifier.set_params(**{allNames[p]: hyperparameter_combo[p]})\n",
    "              \n",
    "        metrics.append(kFoldCrossValidation(classifier, training_set_X, training_set_Y, number_of_folds, type_of_score, kf))\n",
    "\n",
    "    best_params = params[metrics.index(max(metrics))]\n",
    "    return {allNames[i]: best_params[i] for i in range(len(best_params))}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kFoldCrossValidation(classifier, training_set_X, training_set_Y, number_of_folds, type_of_score = 'all', kf = None):\n",
    "    \"\"\"Function that returns cross-validated model performance score\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    -classifer: supervised learning model\n",
    "    -training_set_X: the X data \n",
    "    -training_set_Y: the Y data \n",
    "    -number_of_folds: number of cross validation folds\n",
    "    -type_of_score: performance metric to return, options are \"recall\", \"precision\", \"f1\", \"accuracy\", or \"all\"\n",
    "    -kf: folds to consider\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    Float or list depending on type_of_score argument representing the model's performance\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    kFoldCrossValidation(LogisticRegression(penalty='l2', tol=0.0001, C=1.0) , X, Y, 5, 'recall')\n",
    "    \"\"\"\n",
    "    if kf == None:\n",
    "        kf = KFold(n_splits= number_of_folds)\n",
    "        kf.get_n_splits(training_set_X)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    accuracy = []\n",
    "\n",
    "    for train_index, test_index in kf.split(training_set_X):\n",
    "        X_train, X_validation = training_set_X[train_index], training_set_X[test_index]\n",
    "        Y_train, Y_validation = training_set_Y[train_index], training_set_Y[test_index]\n",
    "\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        y_pred = classifier.predict(X_validation)\n",
    "        prec, rec, f1, sup = precision_recall_fscore_support(Y_validation, y_pred, average= \"binary\")\n",
    "        acc = accuracy_score(Y_validation, y_pred)\n",
    "\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        f1_score.append(f1)\n",
    "        accuracy.append(acc)\n",
    "\n",
    "    mean_precision = np.mean(precision)\n",
    "    mean_recall = np.mean(recall)\n",
    "    mean_f1 = np.mean(f1_score)\n",
    "    mean_accuracy = np.mean(accuracy)\n",
    "    \n",
    "    if type_of_score == \"accuracy\":\n",
    "        return mean_accuracy\n",
    "    if type_of_score == \"precision\":\n",
    "        return mean_precision\n",
    "    if type_of_score == \"f1\":\n",
    "        return mean_f1\n",
    "    if type_of_score == \"recall\":\n",
    "        return mean_recall\n",
    "    else:\n",
    "        return [mean_precision, mean_recall, mean_f1, mean_accuracy]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ouput on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, hyperparameters):\n",
    "    \"\"\"Function that finds test set predictions \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    -model: base supervised learning model\n",
    "    -hyperparameters: dictionary of parameter name to options, like {'C': 1.0, 'penalty': 'l2'}\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    Saves a csv called \"results.csv\"- first column is predicted, second is actual\n",
    "    \n",
    "    Example:\n",
    "    -------\n",
    "    test_model(trainX, testX, trainY, testY, LogisticRegression(), {'C': 1.0, 'penalty': 'l2'})\n",
    "    \"\"\"\n",
    "    \n",
    "    allNames = sorted(hyperparameters)\n",
    "    for p in allNames:\n",
    "        model.set_params(**{p: hyperparameters[p]})\n",
    "    \n",
    "    model.fit(training_set_X, training_set_Y)\n",
    "\n",
    "    y_pred = model.predict(test_set_X)\n",
    "    try:\n",
    "        probabilities = model.predict_proba(test_set_X)\n",
    "        results = [y_pred, test_set_Y, probabilities[:,1]]\n",
    "    except:\n",
    "        results = [y_pred, test_set_Y]\n",
    "        \n",
    "    numpy_results = np.transpose(np.matrix(results))\n",
    "    np.savetxt(\"results.csv\", numpy_results, delimiter=\",\")\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Driver Function For Picking Best Model and Reporting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TuneAndReport(modeltype, hyperparameters, kfold, metric):\n",
    "    \"\"\"Function that uses k-fold cross validation for hyperparameter tuning then\n",
    "    writes results on test set to a csv file\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    -modeltype: base model\n",
    "    -hyperparameters: dictionary of parameter name to options, like {'C': 1.0, 'penalty': 'l2'}\n",
    "    -kfold: how many cross validation folds to use\n",
    "    -metric: metric to evaluate tuning with; options \"recall\", \"precision\", \"f1\", \"accuracy\"\n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "    Saves results.csv file\n",
    "    \n",
    "    \"\"\"\n",
    "    data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "    data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "    \n",
    "\n",
    "    if modeltype == \"LogisticRegression\":\n",
    "        model = LogisticRegression()\n",
    "        \n",
    "    if modeltype == \"RandomForest\":\n",
    "        model = RandomForestClassifier()\n",
    "        \n",
    "    if modeltype == \"knn\":\n",
    "        model = KNeighborsClassifier()\n",
    "        \n",
    "    if modeltype == 'MLP':\n",
    "        model = MLPClassifier()\n",
    "        \n",
    "    if modeltype == \"SVM\":\n",
    "        model = SVC()\n",
    "        \n",
    "    if modeltype == \"AdaBoost\":\n",
    "        model = AdaBoostClassifier()\n",
    "        \n",
    "    if modeltype == \"GradientBoosting\":\n",
    "        model = GradientBoostingClassifier()\n",
    "        \n",
    "    if modeltype == \"LinearSVM\":\n",
    "        model = LinearSVC()\n",
    "\n",
    "    if modeltype == \"DecisionTree\":\n",
    "        model = DecisionTreeClassifier()\n",
    "        \n",
    "    training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "    \n",
    "    best_params = tuneHyperparameters(model, training_set_X, training_set_Y, hyperparameters, kfold, metric)\n",
    "\n",
    "    test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "    \n",
    "    #print best_params\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Example- Tune then Create Result Numpy Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Tune model and pick best hyperparameters\n",
    "parameters = {'C': [0.001,0.01,0.1,0.5,1], 'penalty': ['l1','l2']}\n",
    "model = \"AdaBoost\"\n",
    "TuneAndReport(model, parameters, 5, 'recall')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
