{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tuneHyperparameters(classifier, training_set_X, training_set_Y, hyperparameters, number_of_folds, type_of_score):\n",
    "    \"\"\"This function runs k-fold cross validation on given set of parameters and returns best combo of parameters (according to specified model performance metric)\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    -classifier: base classifer, parameters will be set during this function\n",
    "    -training_set_X: X data\n",
    "    -training_set_Y: Y data\n",
    "    -hyperparameters: dictionary of parameter name to options\n",
    "    -number_of_folds: number of k-fold folds\n",
    "    -type_of_score: performance metric used to compare hyperparameters, options are \"recall\", \"precision\", \"f1\", or \"accuracy\"\n",
    "    \n",
    "    Returns: \n",
    "    -------\n",
    "    Dictionary of best parameter values, for example {'C': 2, 'penalty': 'l2'}\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    parameters = {'C': [3,2,5,6], 'penalty': ['l2']}\n",
    "    tuneHyperparameters(LogisticRegression(), X, Y, parameters, 5, 'accuracy')\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits= number_of_folds)\n",
    "    kf.get_n_splits(training_set_X)\n",
    "    \n",
    "    allNames = sorted(hyperparameters)\n",
    "    parameter_combos = it.product(*(hyperparameters[Name] for Name in allNames))\n",
    "    \n",
    "    metrics = []\n",
    "    params = []\n",
    "    for hyperparameter_combo in parameter_combos:\n",
    "        params.append(hyperparameter_combo)\n",
    "        for p in range(len(allNames)):\n",
    "            classifier.set_params(**{allNames[p]: hyperparameter_combo[p]})\n",
    "              \n",
    "        metrics.append(kFoldCrossValidation(classifier, training_set_X, training_set_Y, number_of_folds, type_of_score, kf))\n",
    "    print metrics\n",
    "    best_params = params[metrics.index(max(metrics))]\n",
    "    return {allNames[i]: best_params[i] for i in range(len(best_params))}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kFoldCrossValidation(classifier, training_set_X, training_set_Y, number_of_folds, type_of_score = 'all', kf = None):\n",
    "    \"\"\"Function that returns cross-validated model performance score\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    -classifer: supervised learning model\n",
    "    -training_set_X: the X data \n",
    "    -training_set_Y: the Y data \n",
    "    -number_of_folds: number of cross validation folds\n",
    "    -type_of_score: performance metric to return, options are \"recall\", \"precision\", \"f1\", \"accuracy\", or \"all\"\n",
    "    -kf: folds to consider\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    Float or list depending on type_of_score argument representing the model's performance\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    kFoldCrossValidation(LogisticRegression(penalty='l2', tol=0.0001, C=1.0) , X, Y, 5, 'recall')\n",
    "    \"\"\"\n",
    "    if kf == None:\n",
    "        kf = KFold(n_splits= number_of_folds)\n",
    "        kf.get_n_splits(training_set_X)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    accuracy = []\n",
    "\n",
    "    for train_index, test_index in kf.split(training_set_X):\n",
    "        X_train, X_validation = training_set_X[train_index], training_set_X[test_index]\n",
    "        Y_train, Y_validation = training_set_Y[train_index], training_set_Y[test_index]\n",
    "\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        y_pred = classifier.predict(X_validation)\n",
    "        prec, rec, f1, sup = precision_recall_fscore_support(Y_validation, y_pred, average= \"binary\")\n",
    "        acc = accuracy_score(Y_validation, y_pred)\n",
    "\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        f1_score.append(f1)\n",
    "        accuracy.append(acc)\n",
    "\n",
    "    mean_precision = np.mean(precision)\n",
    "    mean_recall = np.mean(recall)\n",
    "    mean_f1 = np.mean(f1_score)\n",
    "    mean_accuracy = np.mean(accuracy)\n",
    "    \n",
    "    if type_of_score == \"accuracy\":\n",
    "        return mean_accuracy\n",
    "    if type_of_score == \"precision\":\n",
    "        return mean_precision\n",
    "    if type_of_score == \"f1\":\n",
    "        return mean_f1\n",
    "    if type_of_score == \"recall\":\n",
    "        return mean_recall\n",
    "    else:\n",
    "        return [mean_precision, mean_recall, mean_f1, mean_accuracy]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, hyperparameters):\n",
    "    \"\"\"Function that finds test set predictions \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    -model: base supervised learning model\n",
    "    -hyperparameters: dictionary of parameter name to options, like {'C': 1.0, 'penalty': 'l2'}\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    Saves a csv called \"results.csv\"- first column is predicted, second is actual\n",
    "    \n",
    "    Example:\n",
    "    -------\n",
    "    test_model(trainX, testX, trainY, testY, LogisticRegression(), {'C': 1.0, 'penalty': 'l2'})\n",
    "    \"\"\"\n",
    "    \n",
    "    allNames = sorted(hyperparameters)\n",
    "    for p in allNames:\n",
    "        model.set_params(**{p: hyperparameters[p]})\n",
    "    \n",
    "    model.fit(training_set_X, training_set_Y)\n",
    "    print model.feature_importances_\n",
    "    y_pred = model.predict(test_set_X)\n",
    "    probabilities = model.predict_proba(test_set_X)\n",
    "    \n",
    "    prec, rec, f1, sup = precision_recall_fscore_support(test_set_Y, y_pred, average= \"binary\")\n",
    "    acc = accuracy_score(test_set_Y, y_pred)\n",
    "\n",
    "    print \"Accuracy is:\", acc, \"Recall is:\", rec\n",
    "    results = [y_pred, test_set_Y, probabilities[:,1]]\n",
    "    numpy_results = np.transpose((results))\n",
    "    np.savetxt(\"results_PET.csv\", numpy_results, delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55352615717385145, 0.66295665420301897, 0.56083414116235064, 0.62564959344768234, 0.62318653926049028, 0.62323995489346551]\n",
      "[ 0.01478254  0.1039612   0.38751827  0.07831245  0.01870226  0.05358777\n",
      "  0.2422466   0.06382392  0.          0.00497629  0.01727738  0.01481132]\n",
      "Accuracy is: 0.780487804878 Recall is: 0.764227642276\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_train = pd.read_csv('ImputedMatrix_train.csv', header = 0)\n",
    "data_test = pd.read_csv('ImputedMatrix_test.csv', header = 0)\n",
    "\n",
    "data_train = data_train[['FDG_MaxTime', 'FDG_Delta', 'FDG_Mean','FDG_Std', 'AV45_MaxTime','AV45_Delta', 'AV45_Mean', 'AV45_Std','PIB_MaxTime','PIB_Delta', 'PIB_Mean', 'PIB_Std', 'Diagnostics']]\n",
    "data_test = data_test[['FDG_MaxTime', 'FDG_Delta', 'FDG_Mean','FDG_Std', 'AV45_MaxTime','AV45_Delta', 'AV45_Mean', 'AV45_Std', 'PIB_MaxTime','PIB_Delta', 'PIB_Mean', 'PIB_Std','Diagnostics']]\n",
    "\n",
    "training_set_Y = np.array(data_train['Diagnostics'].tolist())\n",
    "test_set_Y = np.array(data_test['Diagnostics'].tolist())\n",
    "\n",
    "del data_train['Diagnostics']\n",
    "del data_test['Diagnostics']\n",
    "\n",
    "training_set_X = np.matrix(data_train)\n",
    "test_set_X = np.matrix(data_test)\n",
    "\n",
    "best_params = tuneHyperparameters(RandomForestClassifier(), training_set_X, training_set_Y, {'n_estimators': [10,50,100,200,300,500]}, 5, 'recall')\n",
    "\n",
    "test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, RandomForestClassifier(), best_params)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
