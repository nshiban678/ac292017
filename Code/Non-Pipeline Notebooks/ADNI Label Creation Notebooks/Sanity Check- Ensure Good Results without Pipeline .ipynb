{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import csv\n",
    "import pandas as pd \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/courtneycochrane/anaconda/envs/py27/lib/python2.7/site-packages/nbformat/current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.worksheets[0].cells:\n",
    "                if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Random Forest model with just mean mode imputation, no categorical encoding, no feature selection for all available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get labels\n",
    "\n",
    "#DX_CURREN = {'1':'NL', \"2\": 'MCI', \"3\": 'AD', \"\":\"\"}\n",
    "DX_CURREN = {\"1\": 0, \"2\": 0, \"3\": 1, \"\":\"\"}\n",
    "#DX_CHANGE = {'1':\"Stable:NL to NL\",'2':\"Stable: MCI to MCI\",'3':\"Stable: AD to AD\",'4':\"Conv:NL to MCI\",'5':\"Conv:MCI to AD\",'6':\"Conv:NL to AD\", '7':\"Rev:MCI to NL\",'8':\"Rev:AD to MCI\",'9':\"Conv:AD to NL\",\"\":\"\"}\n",
    "DX_CHANGE = {\"1\":0,\"2\":0,\"3\":1,\"4\":0,\"5\":1,\"6\":1,\"7\":0,\"8\":0,\"9\":0,\"\":\"\"}\n",
    "\n",
    "reverted_patients = [167, 429, 555, 1226, 2210, 2367, 4005, 4114, 4426, 4434, 4641, 4706, 4746, 4899]\n",
    "\n",
    "patient_diagnosis_dict = {}\n",
    "patients_nonADdementia = set()\n",
    "\n",
    "with open('../Assessments/DXSUM_PDXCONV_ADNIALL.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader) #skip header\n",
    "    for row in reader:\n",
    "        RID = int(row[2])\n",
    "        EXAMDATE = datetime.strptime(row[8], '%m/%d/%Y')\n",
    "        Dx_curren = row[10]\n",
    "        Dx_change = row[9]\n",
    "        DXOTHDEM = row[47]\n",
    "\n",
    "        if RID not in patient_diagnosis_dict:\n",
    "            patient_diagnosis_dict[RID] = []\n",
    "\n",
    "        #use the DXCURREN or DXCHANGE, depending on which is present\n",
    "        if Dx_curren != \"\" and Dx_change == \"\":\n",
    "            patient_diagnosis_dict[RID].append([EXAMDATE, DX_CURREN[Dx_curren]])\n",
    "        elif Dx_change != \"\" and Dx_curren == \"\":\n",
    "            patient_diagnosis_dict[RID].append([EXAMDATE, DX_CHANGE[Dx_change]])\n",
    "        else:\n",
    "            assert 1 == 0\n",
    "\n",
    "        #Check for the Non-AD dementia cases\n",
    "        ##################\n",
    "        #if Dx_change indicates AD but non-AD dementia by DXOTHDEM\n",
    "        if DXOTHDEM == \"1\":\n",
    "            if Dx_change in ['3','5','6'] or Dx_curren == '3':\n",
    "                patients_nonADdementia.add(RID)\n",
    "\n",
    "#take the most recent diagnosis information \n",
    "for patient in patient_diagnosis_dict:\n",
    "    exams = sorted(patient_diagnosis_dict[patient])[-1]\n",
    "    patient_diagnosis_dict[patient] = exams\n",
    "\n",
    "patient_dict = []\n",
    "labels = []\n",
    "response_variable = []\n",
    "\n",
    "with open('../Data___Database/ADNIMERGE.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        if row[0] == \"RID\":\n",
    "            labels = row\n",
    "        else:\n",
    "            assert patient_diagnosis_dict[int(row[0])] #checks to make sure this patient is in diagnosis file\n",
    "\n",
    "            diag_info = patient_diagnosis_dict[int(row[0])]\n",
    "            diag_date = diag_info[0]\n",
    "            diag = diag_info[1]\n",
    "\n",
    "            if int(row[0]) in patients_nonADdementia: #manually correct patients with non-AD dementia \n",
    "                response_variable.append(0)\n",
    "\n",
    "            #AD patient if the most recent Diagnosis is AD\n",
    "            elif diag == 1:\n",
    "                response_variable.append(1)\n",
    "            else:\n",
    "                response_variable.append(0)\n",
    "\n",
    "\n",
    "            #parameter setting to label reversions as AD = 0\n",
    "            if int(row[0]) not in reverted_patients:\n",
    "                patient_dict.append(row)\n",
    "            else:\n",
    "                response_variable = response_variable[:-1]\n",
    "\n",
    "#write labels out to a separate file\n",
    "with open('FinalLabels.csv','wb') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for i in range(len(patient_dict)):\n",
    "        writer.writerow([patient_dict[i][0],response_variable[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert to DataFrame and delete DX column\n",
    "df = pd.DataFrame(np.array(patient_dict))\n",
    "df.columns = labels\n",
    "del df['DX']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Variables and Delete Variables We Don't Want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Converts the two ExamDate columns into one EXAMDATE_bl_minus_EXAMDATE column\n",
    "date_cols = ['EXAMDATE','EXAMDATE_bl']\n",
    "df['EXAMDATE_bl_minus_EXAMDATE'] = (pd.to_datetime(df[date_cols[0]])-pd.to_datetime(df[date_cols[1]]))\n",
    "   \n",
    "delta = []\n",
    "for i in df['EXAMDATE_bl_minus_EXAMDATE']:\n",
    "    delta.append(i.days) #append the number of days since the basline visit\n",
    "df['EXAMDATE_bl_minus_EXAMDATE'] = delta\n",
    "\n",
    "\n",
    "\n",
    "###Delete these given columns now \n",
    "cols_to_delete = ['update_stamp','EXAMDATE','EXAMDATE_bl','PTID','SITE','FLDSTRENG','FSVERSION','FLDSTRENG_bl','FSVERSION_bl','DX_bl']\n",
    "for i in cols_to_delete:\n",
    "    del df[i]\n",
    "\n",
    "    \n",
    "    \n",
    "####one-hot encode the following columns \n",
    "one_hot_cols = ['VISCODE','COLPROT','ORIGPROT','PTGENDER','PTETHCAT','PTRACCAT','PTMARRY']\n",
    "df = pd.get_dummies(df, columns = one_hot_cols)\n",
    "\n",
    "df.to_csv(\"Merged_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longitudinal Code (adapted slightly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from LongitudinalDataAnalysis_WithOutLabels.ipynb\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import LongitudinalDataAnalysis_WithOutLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>FDG</th>\n",
       "      <th>PIB</th>\n",
       "      <th>AV45</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS11</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>...</th>\n",
       "      <th>PTRACCAT_Black</th>\n",
       "      <th>PTRACCAT_Hawaiian/Other PI</th>\n",
       "      <th>PTRACCAT_More than one</th>\n",
       "      <th>PTRACCAT_Unknown</th>\n",
       "      <th>PTRACCAT_White</th>\n",
       "      <th>PTMARRY_Divorced</th>\n",
       "      <th>PTMARRY_Married</th>\n",
       "      <th>PTMARRY_Never married</th>\n",
       "      <th>PTMARRY_Unknown</th>\n",
       "      <th>PTMARRY_Widowed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>74.3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.36926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>18.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>81.3</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>81.3</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.06360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RID   AGE  PTEDUCAT  APOE4      FDG  PIB  AV45  CDRSB  ADAS11  ADAS13  \\\n",
       "0    2  74.3        16    0.0  1.36926  NaN   NaN    0.0   10.67   18.67   \n",
       "1    3  81.3        18    1.0  1.09079  NaN   NaN    4.5   22.00   31.00   \n",
       "2    3  81.3        18    1.0  1.06360  NaN   NaN    6.0   19.00   30.00   \n",
       "\n",
       "        ...         PTRACCAT_Black  PTRACCAT_Hawaiian/Other PI  \\\n",
       "0       ...                    0.0                         0.0   \n",
       "1       ...                    0.0                         0.0   \n",
       "2       ...                    0.0                         0.0   \n",
       "\n",
       "   PTRACCAT_More than one  PTRACCAT_Unknown  PTRACCAT_White  PTMARRY_Divorced  \\\n",
       "0                     0.0               0.0             1.0               0.0   \n",
       "1                     0.0               0.0             1.0               0.0   \n",
       "2                     0.0               0.0             1.0               0.0   \n",
       "\n",
       "   PTMARRY_Married  PTMARRY_Never married  PTMARRY_Unknown  PTMARRY_Widowed  \n",
       "0              1.0                    0.0              0.0              0.0  \n",
       "1              1.0                    0.0              0.0              0.0  \n",
       "2              1.0                    0.0              0.0              0.0  \n",
       "\n",
       "[3 rows x 122 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix Size:\n",
      "-----------------------\n",
      "(12618, 122)\n",
      " \n",
      "Identified columns of interest in input file: \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Patient RID column: [0]\n",
      "Demo columns: [  1   2 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120\n",
      " 121]\n",
      "BaselineOneTime columns: [ 3 72]\n",
      "BaselineEvaluation columns: [38 39 40 41 43 44 45 46 54 56 57 58 59 60 61 63 64 65 66 67 68]\n",
      "Time columns: [ 73  74  75  76  99 100 101 102 103 104]\n",
      "CurrentEvaluation columns: [ 7  8  9 10 12 13 14 15 16 18 19 20 21 22 23 25 26 27 28 29 30]\n",
      "------\n",
      "Method 2 for Longitudinal Data Analysis\n",
      "------\n",
      " \n",
      "New Input Matrix Size:\n",
      "-----------------------\n",
      "(1723, 106)\n",
      " \n",
      "New Output Matrix Size:\n",
      "-----------------------\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Longitudinal Data Analysis\n",
    "# -------------------------------\n",
    "\n",
    "# Input file name for Longitudinal Data Analysis\n",
    "InputToLongitudinal='Merged_data.csv'\n",
    "\n",
    "with open(InputToLongitudinal) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    labels = next(reader)\n",
    "\n",
    "# Output file name from this script\n",
    "OutputFromLongitudinal='LongitudinalDataAnalysis.csv'\n",
    "\n",
    "# Patient RID Features\n",
    "Patient_FEATURES=['RID'];\n",
    "\n",
    "# Demographic Features\n",
    "Demo_FEATURES_type=['AGE','PTEDUCAT','PTGENDER','PTETHCAT','PTRACCAT','PTMARRY']\n",
    "Demo_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    \n",
    "# Baseline OneTime Features\n",
    "BaselineOneTime_FEATURES_type = ['APOE4','Years_bl','DX_bl']\n",
    "BaselineOneTime_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "    elif i.rfind(\"_\") != -1 and i[:i.rfind(\"_\")] in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "        \n",
    "# Time Headers\n",
    "Time_FEATURES_type=['SITE','Month','update_stamp_minus_EXAMDATE_bl','update_stamp_minus_EXAMDATE','EXAMDATE_bl_minus_EXAMDATE',\n",
    "               'COLPROT','ORIGPROT','M','Month_bl']\n",
    "Time_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "\n",
    "# Baseline Evaluation Features\n",
    "BaselineEvaluation_FEATURES=['CDRSB_bl','ADAS11_bl','ADAS13_bl','MMSE_bl','RAVLT_learning_bl','RAVLT_forgetting_bl',\n",
    "                             'RAVLT_perc_forgetting_bl','FAQ_bl','MOCA_bl','EcogPtLang_bl','EcogPtVisspat_bl',\n",
    "                             'EcogPtPlan_bl','EcogPtOrgan_bl','EcogPtDivatt_bl','EcogPtTotal_bl','EcogSPLang_bl',\n",
    "                             'EcogSPVisspat_bl','EcogSPPlan_bl','EcogSPOrgan_bl','EcogSPDivatt_bl','EcogSPTotal_bl'];\n",
    "\n",
    "\n",
    "   \n",
    "# Current Medical Evaluation\n",
    "CurrentEvaluation_FEATURES=['CDRSB','ADAS11','ADAS13','MMSE','RAVLT_learning','RAVLT_forgetting','RAVLT_perc_forgetting',\n",
    "                            'FAQ','MOCA','EcogPtLang','EcogPtVisspat','EcogPtPlan','EcogPtOrgan','EcogPtDivatt','EcogPtTotal',\n",
    "                            'EcogSPLang','EcogSPVisspat','EcogSPPlan','EcogSPOrgan','EcogSPDivatt','EcogSPTotal'];\n",
    "\n",
    "\n",
    "# Longitudinal Method\n",
    "LongitudinalMethod=2;\n",
    "MetricList=['MaxTime','Delta','Mean','Std'];\n",
    "\n",
    "# Run Longitudinal Data Anaysis\n",
    "LongitudinalDataAnalysis_WithOutLabels.runLongitudinal(InputToLongitudinal,OutputFromLongitudinal,Patient_FEATURES,Demo_FEATURES,\\\n",
    "                                         BaselineOneTime_FEATURES,Time_FEATURES,BaselineEvaluation_FEATURES,\\\n",
    "                                         CurrentEvaluation_FEATURES,LongitudinalMethod,MetricList)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Deal with X \n",
    "df = pd.read_csv('LongitudinalDataAnalysis.csv')\n",
    "l = len(df)\n",
    "data_train = df.iloc[:int(2*l/3.0),:]\n",
    "data_test = df.iloc[int(2*l/3.0)+1:,:]\n",
    "\n",
    "#Read in Y from the saved file earlier\n",
    "patient_labels = {}\n",
    "with open('FinalLabels.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        patient_labels[int(row[0])] = int(row[1])\n",
    "\n",
    "Y_train = [patient_labels[i] for i in list(data_train.iloc[:,0])]\n",
    "Y_test = [patient_labels[i] for i in list(data_test.iloc[:,0])]\n",
    "\n",
    "data_train.to_csv(\"LongitudinalDataAnalysis_train.csv\", index = False)\n",
    "data_test.to_csv(\"LongitudinalDataAnalysis_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-mode Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"LongitudinalDataAnalysis_train.csv\")\n",
    "data_test = pd.read_csv(\"LongitudinalDataAnalysis_test.csv\")\n",
    "\n",
    "for column in data_train:\n",
    "    #convert to numeric\n",
    "    data_train[[column]] = data_train[[column]].apply(pd.to_numeric)\n",
    "    data_test[[column]] = data_test[[column]].apply(pd.to_numeric)\n",
    "    \n",
    "    #if the column empty, delete it \n",
    "    if pd.isnull(data_train[column]).all():\n",
    "        del data_train[column]    \n",
    "        del data_test[column] \n",
    "\n",
    "    #if this is a categorical column\n",
    "    elif np.array_equal(sorted(data_train[column].unique()),[0,1]) or np.array_equal(sorted(data_train[column].unique()),[0]) or np.array_equal(sorted(data_train[column].unique()),[1]):\n",
    "        data_train[column] = data_train[column].replace(np.nan, data_train[column].value_counts()[0])\n",
    "        data_test[column] = data_test[column].replace(np.nan, data_train[column].value_counts()[0])\n",
    "\n",
    "    else: #if numerical column\n",
    "        data_train[column] = data_train[column].replace(np.nan, data_train[column].mean()) \n",
    "        data_test[column] = data_test[column].replace(np.nan, data_train[column].mean()) \n",
    "\n",
    "data_train.to_csv(\"Features_train.csv\", index = False)\n",
    "data_test.to_csv(\"Features_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.939024390244 Precision: 0.914438502674 Recall: 0.9 F1 0.907161803714\n",
      "Accuracy: 0.925087108014 Precision: 0.940119760479 Recall: 0.826315789474 F1 0.879551820728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "###Read in data and delete RID column before supervised learning\n",
    "data_train = pd.read_csv('Features_train.csv', delimiter=\",\")\n",
    "data_test = pd.read_csv('Features_test.csv', delimiter=\",\")\n",
    "\n",
    "del data_train['# RID']\n",
    "del data_test['# RID']\n",
    "##########\n",
    "\n",
    "\n",
    "## MLP Model\n",
    "model = MLPClassifier()\n",
    "model.fit(data_train, Y_train)\n",
    "\n",
    "y_pred = model.predict(data_test)\n",
    "\n",
    "prec, rec, f1, sup = precision_recall_fscore_support(Y_test, y_pred, average= 'binary')\n",
    "acc = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print \"Accuracy:\",acc, \"Precision:\",prec, \"Recall:\",rec, \"F1\",f1\n",
    "\n",
    "## Random Forest Model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(data_train, Y_train)\n",
    "featimp = model.feature_importances_\n",
    "\n",
    "importances = sorted(zip(featimp,list(data_train)),reverse = True)\n",
    "\n",
    "y_pred = model.predict(data_test)\n",
    "\n",
    "prec, rec, f1, sup = precision_recall_fscore_support(Y_test, y_pred, average= 'binary')\n",
    "acc = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print \"Accuracy:\",acc, \"Precision:\",prec, \"Recall:\",rec, \"F1\",f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.13886068807876101, 'FAQ_Mean')\n",
      "(0.098756345708504084, 'RAVLT_perc_forgetting_Mean')\n",
      "(0.094684954472393654, 'MMSE_Mean')\n",
      "(0.090463608097144349, 'ADAS11_Mean')\n",
      "(0.086292068433668781, 'ADAS13_Mean')\n",
      "(0.065302518267442058, 'EcogSPTotal_Mean')\n",
      "(0.04738300247302956, 'CDRSB_Mean')\n",
      "(0.043484439230889335, 'CDRSB_Delta')\n",
      "(0.041917960667983992, 'CDRSB_Std')\n",
      "(0.034498462215677012, 'EcogSPVisspat_Mean')\n",
      "(0.022891836611709439, 'FAQ_Std')\n",
      "(0.015445954125760396, 'RAVLT_learning_Mean')\n",
      "(0.012573139481013962, 'ADAS11_Std')\n",
      "(0.011647826970743109, 'ADAS11_Delta')\n",
      "(0.011271858508419857, 'EcogSPOrgan_Mean')\n",
      "(0.010913428401122948, 'ADAS13_Std')\n",
      "(0.010537301556634061, 'ADAS13_Delta')\n",
      "(0.0087227894491112533, 'AGE')\n",
      "(0.0073421192395510928, 'RAVLT_forgetting_Delta')\n",
      "(0.0072199176404592752, 'MMSE_Std')\n",
      "(0.0065026539084536827, 'RAVLT_perc_forgetting_Std')\n",
      "(0.0063494007083218559, 'FAQ_Delta')\n",
      "(0.0062596082176310745, 'MMSE_Delta')\n",
      "(0.0062289284537240374, 'EcogSPPlan_Mean')\n",
      "(0.0061778583186433127, 'EcogSPDivatt_Mean')\n",
      "(0.0051162191810591754, 'RAVLT_forgetting_Mean')\n",
      "(0.0050111625623678688, 'RAVLT_learning_Std')\n",
      "(0.0048012227436680143, 'RAVLT_learning_Delta')\n",
      "(0.0047765869417512579, 'EcogSPLang_Mean')\n",
      "(0.0045348037034880348, 'RAVLT_forgetting_Std')\n",
      "(0.0040733141627975035, 'EcogSPDivatt_Std')\n",
      "(0.0036789933687534611, 'PTEDUCAT')\n",
      "(0.0034245616072759747, 'RAVLT_perc_forgetting_Delta')\n",
      "(0.0033743504583948944, 'EcogSPPlan_Std')\n",
      "(0.0032922890843046135, 'EcogSPPlan_Delta')\n",
      "(0.0032147490124395995, 'EcogSPVisspat_Delta')\n",
      "(0.0031588889331140612, 'APOE4')\n",
      "(0.0031164940812572648, 'EcogPtOrgan_Mean')\n",
      "(0.0026646274251926486, 'EcogSPOrgan_Delta')\n",
      "(0.0026598993414632191, 'EcogSPVisspat_Std')\n",
      "(0.0022863195391596811, 'EcogPtTotal_Mean')\n",
      "(0.0022775958054064285, 'EcogPtTotal_Delta')\n",
      "(0.0022160119397739457, 'Years_bl')\n",
      "(0.0021912355693661026, 'MOCA_Std')\n",
      "(0.0020603323401508049, 'EcogSPLang_Std')\n",
      "(0.0020357023448160693, 'EcogPtTotal_MaxTime')\n",
      "(0.0019871730269326975, 'EcogPtOrgan_Std')\n",
      "(0.0018818760340329032, 'EcogPtTotal_Std')\n",
      "(0.0017847959177428204, 'EcogPtLang_Mean')\n",
      "(0.0017609387629412706, 'EcogSPTotal_Delta')\n",
      "(0.0017336336811794584, 'MOCA_Mean')\n",
      "(0.0016304153541994465, 'PTGENDER_Female')\n",
      "(0.0014208706086075126, 'EcogPtLang_Delta')\n",
      "(0.001396301092218578, 'ADAS11_MaxTime')\n",
      "(0.0013183933296997716, 'EcogPtLang_Std')\n",
      "(0.0012889477178515498, 'EcogPtVisspat_Delta')\n",
      "(0.0012692301869553118, 'EcogPtDivatt_Std')\n",
      "(0.0012006683151927612, 'PTGENDER_Male')\n",
      "(0.0011532997524109849, 'FAQ_MaxTime')\n",
      "(0.0011214448449279745, 'EcogSPTotal_MaxTime')\n",
      "(0.0010761215450302392, 'EcogPtPlan_Mean')\n",
      "(0.0010318874825890082, 'PTRACCAT_Black')\n",
      "(0.00098401998106075741, 'EcogPtVisspat_Mean')\n",
      "(0.00096458000873881375, 'EcogPtPlan_Std')\n",
      "(0.00088197314983584608, 'EcogSPTotal_Std')\n",
      "(0.00087577696676961378, 'EcogPtDivatt_Mean')\n",
      "(0.00087326680831987624, 'EcogSPDivatt_Delta')\n",
      "(0.00084714238898641009, 'PTMARRY_Widowed')\n",
      "(0.00081957714776546213, 'PTRACCAT_Asian')\n",
      "(0.00080902177581548395, 'EcogSPDivatt_MaxTime')\n",
      "(0.00069902688871783581, 'EcogPtVisspat_MaxTime')\n",
      "(0.00065222927857954663, 'EcogSPVisspat_MaxTime')\n",
      "(0.0005921494200054403, 'PTMARRY_Divorced')\n",
      "(0.00052524354438573441, 'EcogPtDivatt_Delta')\n",
      "(0.00051037356499669608, 'PTETHCAT_Hisp/Latino')\n",
      "(0.0005032589082964073, 'EcogSPLang_MaxTime')\n",
      "(0.00049499553301399072, 'EcogPtOrgan_Delta')\n",
      "(0.00048206543185980183, 'PTMARRY_Married')\n",
      "(0.00041729975680758881, 'EcogSPOrgan_Std')\n",
      "(0.00035562175029206749, 'EcogPtLang_MaxTime')\n",
      "(0.00035372039732119425, 'PTRACCAT_White')\n",
      "(0.00032842538907537394, 'MMSE_MaxTime')\n",
      "(0.00031620331092851062, 'EcogPtVisspat_Std')\n",
      "(0.00030126346367052322, 'PTRACCAT_Am Indian/Alaskan')\n",
      "(0.00028858722976370029, 'EcogSPPlan_MaxTime')\n",
      "(0.00028424279422265897, 'PTETHCAT_Not Hisp/Latino')\n",
      "(0.00027642640910345421, 'EcogPtPlan_MaxTime')\n",
      "(0.00024731453352186286, 'MOCA_Delta')\n",
      "(0.00024404761904761908, 'EcogSPOrgan_MaxTime')\n",
      "(0.0001696239622755143, 'RAVLT_learning_MaxTime')\n",
      "(0.00015249555151615216, 'EcogPtOrgan_MaxTime')\n",
      "(0.0, 'RAVLT_perc_forgetting_MaxTime')\n",
      "(0.0, 'RAVLT_forgetting_MaxTime')\n",
      "(0.0, 'PTRACCAT_Unknown')\n",
      "(0.0, 'PTRACCAT_More than one')\n",
      "(0.0, 'PTRACCAT_Hawaiian/Other PI')\n",
      "(0.0, 'PTMARRY_Unknown')\n",
      "(0.0, 'PTMARRY_Never married')\n",
      "(0.0, 'PTETHCAT_Unknown')\n",
      "(0.0, 'MOCA_MaxTime')\n",
      "(0.0, 'EcogSPLang_Delta')\n",
      "(0.0, 'EcogPtPlan_Delta')\n",
      "(0.0, 'EcogPtDivatt_MaxTime')\n",
      "(0.0, 'CDRSB_MaxTime')\n",
      "(0.0, 'ADAS13_MaxTime')\n"
     ]
    }
   ],
   "source": [
    "for i in importances:\n",
    "    print i"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
