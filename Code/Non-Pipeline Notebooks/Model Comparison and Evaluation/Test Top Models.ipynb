{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/courtneycochrane/anaconda/envs/py27/lib/python2.7/site-packages/nbformat/current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.worksheets[0].cells:\n",
    "                if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MergeDiagnosis_AdjustedLabels.ipynb\n",
      "importing Jupyter notebook from LongitudinalDataAnalysis.ipynb\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Categorical_Updated.ipynb\n",
      "importing Jupyter notebook from Imputation.ipynb\n",
      "importing Jupyter notebook from FeatureReduction.ipynb\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from SupervisedLearning.ipynb\n",
      "importing Jupyter notebook from ModelPerformance.ipynb\n",
      "importing Jupyter notebook from TrainTestSplit.ipynb\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import pandas\n",
    "import csv\n",
    "import MergeDiagnosis_AdjustedLabels\n",
    "import LongitudinalDataAnalysis\n",
    "import Categorical_Updated\n",
    "import Imputation\n",
    "import FeatureReduction\n",
    "import SupervisedLearning\n",
    "import ModelPerformance\n",
    "import TrainTestSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Merge Data\n",
    "# -------------------------------\n",
    "merged_data = MergeDiagnosis_AdjustedLabels.data_preprocess(study = \"all\",imaging_to_drop = 'all', reversions = 'label0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columsn that are one-hot encoded\n",
      "-------------------------------------\n",
      "['VISCODE', 'COLPROT', 'ORIGPROT', 'DX_bl', 'PTGENDER', 'PTETHCAT', 'PTRACCAT', 'PTMARRY']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Categorical to Numerical\n",
    "# -------------------------------\n",
    "date_cols = ['update_stamp','EXAMDATE','EXAMDATE_bl']\n",
    "cols_to_ignore = ['PTID']\n",
    "\n",
    "Categorical_Updated.categorical_conversion(date_cols,cols_to_ignore)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>RID</th>\n",
       "      <th>SITE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS11</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>...</th>\n",
       "      <th>PTRACCAT_Black</th>\n",
       "      <th>PTRACCAT_Hawaiian/Other PI</th>\n",
       "      <th>PTRACCAT_More than one</th>\n",
       "      <th>PTRACCAT_Unknown</th>\n",
       "      <th>PTRACCAT_White</th>\n",
       "      <th>PTMARRY_Married</th>\n",
       "      <th>PTMARRY_Never married</th>\n",
       "      <th>PTMARRY_Unknown</th>\n",
       "      <th>PTMARRY_Widowed</th>\n",
       "      <th>AD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>74.3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>18.67</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>81.3</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>81.3</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  RID  SITE   AGE  PTEDUCAT  APOE4  CDRSB  ADAS11  ADAS13  MMSE  \\\n",
       "0           0    2    11  74.3        16    0.0    0.0   10.67   18.67  28.0   \n",
       "1           1    3    11  81.3        18    1.0    4.5   22.00   31.00  20.0   \n",
       "2           2    3    11  81.3        18    1.0    6.0   19.00   30.00  24.0   \n",
       "\n",
       "  ...  PTRACCAT_Black  PTRACCAT_Hawaiian/Other PI  PTRACCAT_More than one  \\\n",
       "0 ...             0.0                         0.0                     0.0   \n",
       "1 ...             0.0                         0.0                     0.0   \n",
       "2 ...             0.0                         0.0                     0.0   \n",
       "\n",
       "   PTRACCAT_Unknown  PTRACCAT_White  PTMARRY_Married  PTMARRY_Never married  \\\n",
       "0               0.0             1.0              1.0                    0.0   \n",
       "1               0.0             1.0              1.0                    0.0   \n",
       "2               0.0             1.0              1.0                    0.0   \n",
       "\n",
       "   PTMARRY_Unknown  PTMARRY_Widowed  AD  \n",
       "0              0.0              0.0   0  \n",
       "1              0.0              0.0   1  \n",
       "2              0.0              0.0   1  \n",
       "\n",
       "[3 rows x 104 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix Size:\n",
      "-----------------------\n",
      "(12736, 104)\n",
      " \n",
      "Identified columns of interest in input file: \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Patient RID column: [1]\n",
      "Demo columns: [  3   4  90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "BaselineOneTime columns: [ 5 54 84 85]\n",
      "BaselineEvaluation columns: [30 31 32 33 35 36 37 34 38 39 41 42 43 44 45 40 46 48 49 50 51 52 47 53]\n",
      "Time columns: [ 2 56 57 58 59 60 82 83 55]\n",
      "CurrentEvaluation columns: [ 6  7  8  9 11 12 13 10 14 15 17 18 19 20 21 16 22 24 25 26 27 28 23 29]\n",
      "CurrentDiagnosis columns: [103]\n",
      "------\n",
      "Method 2 for Longitudinal Data Analysis\n",
      "------\n",
      " \n",
      "New Input Matrix Size:\n",
      "-----------------------\n",
      "(1737, 115)\n",
      " \n",
      "New Output Matrix Size:\n",
      "-----------------------\n",
      "(1737,)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Longitudinal Data Analysis\n",
    "# -------------------------------\n",
    "\n",
    "# Input file name for Longitudinal Data Analysis\n",
    "InputToLongitudinal='CategoricalToNumerical.csv'\n",
    "\n",
    "with open(InputToLongitudinal) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    labels = next(reader)\n",
    "\n",
    "# Output file name from this script\n",
    "OutputFromLongitudinal='LongitudinalDataAnalysis.csv'\n",
    "\n",
    "# Patient RID Features\n",
    "Patient_FEATURES=['RID'];\n",
    "\n",
    "# Demographic Features\n",
    "Demo_FEATURES_type=['AGE','PTEDUCAT','PTGENDER','PTETHCAT','PTRACCAT','PTMARRY']\n",
    "Demo_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    \n",
    "# Baseline OneTime Features\n",
    "BaselineOneTime_FEATURES_type = ['APOE4','Years_bl','ORIGPROT']\n",
    "BaselineOneTime_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "    elif i.rfind(\"_\") != -1 and i[:i.rfind(\"_\")] in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "        \n",
    "# Time Headers\n",
    "Time_FEATURES_type=['SITE','Month','update_stamp_minus_EXAMDATE_bl','update_stamp_minus_EXAMDATE','EXAMDATE_minus_EXAMDATE_bl',\n",
    "               'COLPROT','M','Month_bl']\n",
    "Time_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "Time_FEATURES.insert(len(Time_FEATURES), Time_FEATURES.pop(Time_FEATURES.index('Month_bl'))) # Month_bl must be last feature in this list\n",
    "\n",
    "        \n",
    "# Baseline Evaluation Features\n",
    "BaselineEvaluation_FEATURES=['CDRSB_bl','ADAS11_bl','ADAS13_bl','MMSE_bl','RAVLT_learning_bl','RAVLT_forgetting_bl',\n",
    "                             'RAVLT_perc_forgetting_bl','RAVLT_immediate_bl','FAQ_bl','MOCA_bl','EcogPtLang_bl','EcogPtVisspat_bl',\n",
    "                             'EcogPtPlan_bl','EcogPtOrgan_bl','EcogPtDivatt_bl','EcogPtMem_bl','EcogPtTotal_bl','EcogSPLang_bl',\n",
    "                             'EcogSPVisspat_bl','EcogSPPlan_bl','EcogSPOrgan_bl','EcogSPDivatt_bl','EcogSPMem_bl','EcogSPTotal_bl'];\n",
    "\n",
    "\n",
    "   \n",
    "# Current Medical Evaluation\n",
    "CurrentEvaluation_FEATURES=['CDRSB','ADAS11','ADAS13','MMSE','RAVLT_learning','RAVLT_forgetting','RAVLT_perc_forgetting','RAVLT_immediate',\n",
    "                            'FAQ','MOCA','EcogPtLang','EcogPtVisspat','EcogPtPlan','EcogPtOrgan','EcogPtDivatt','EcogPtMem','EcogPtTotal',\n",
    "                            'EcogSPLang','EcogSPVisspat','EcogSPPlan','EcogSPOrgan','EcogSPDivatt','EcogSPMem','EcogSPTotal'];\n",
    "\n",
    "\n",
    "# Current Diagnosis\n",
    "CurrentDiagnosis_FEATURES= ['AD'];\n",
    "\n",
    "# Longitudinal Method\n",
    "LongitudinalMethod=2;\n",
    "MetricList=['MaxTime','Delta','Mean','Std'];\n",
    "\n",
    "# Run Longitudinal Data Anaysis\n",
    "LongitudinalDataAnalysis.runLongitudinal(InputToLongitudinal,OutputFromLongitudinal,Patient_FEATURES,Demo_FEATURES,\\\n",
    "                                         BaselineOneTime_FEATURES,Time_FEATURES,BaselineEvaluation_FEATURES,\\\n",
    "                                         CurrentEvaluation_FEATURES,CurrentDiagnosis_FEATURES,LongitudinalMethod,MetricList)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(50):\n",
    "    TrainTestSplit.traintest_split(0.33)\n",
    "\n",
    "    Imputation.imputation('meanmode')\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Feature Reduction \n",
    "    # ----------------------------------\n",
    "\n",
    "    # Input and Output files from Feature Reduction for train set\n",
    "    InputToFeatureReduction_train     ='ImputedMatrix_train.csv'\n",
    "    OutputFromFeatureReduction_train  ='Features_train.csv'\n",
    "    InputToFeatureReduction_test      ='ImputedMatrix_test.csv'\n",
    "    OutputFromFeatureReduction_test   ='Features_test.csv'\n",
    "\n",
    "\n",
    "    # Normalization method\n",
    "    NormalizationMethod='MinMax'\n",
    "    #NormalizationMethod='MeanStd'\n",
    "\n",
    "    # Feature Reduction Method and Settings\n",
    "    #FeatureReductionMethod='SVD'; \n",
    "    ExplainedVariance=0.99; # For method 'SVD'\n",
    "\n",
    "    FeatureReductionMethod= 'none'; \n",
    "    APpreference=-50; # Hyperparameter for method 'AffinityPropagation'\n",
    "\n",
    "\n",
    "    # Run Feature Reduction\n",
    "    FeatureReduction.RunFeatureReduction(InputToFeatureReduction_train,OutputFromFeatureReduction_train,\\\n",
    "                                         InputToFeatureReduction_test,OutputFromFeatureReduction_test,\\\n",
    "                                         NormalizationMethod,FeatureReductionMethod,ExplainedVariance,APpreference)\n",
    "\n",
    "\n",
    "    data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "    data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "    \n",
    "    training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "\n",
    "    best_params = {'alpha': 0.05, 'hidden_layer_sizes': (100,100,100)}\n",
    "    model = MLPClassifier()\n",
    "    SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "    res.append(ModelPerformance.model_performance('all'))\n",
    "    print i\n",
    "\n",
    "with open(\"Model1_Results_NEW.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for m in range(len(res)):\n",
    "        writer.writerow([res[m]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(50):\n",
    "    TrainTestSplit.traintest_split(0.33)\n",
    "\n",
    "    Imputation.imputation('knn')\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Feature Reduction \n",
    "    # ----------------------------------\n",
    "\n",
    "    # Input and Output files from Feature Reduction for train set\n",
    "    InputToFeatureReduction_train     ='ImputedMatrix_train.csv'\n",
    "    OutputFromFeatureReduction_train  ='Features_train.csv'\n",
    "    InputToFeatureReduction_test      ='ImputedMatrix_test.csv'\n",
    "    OutputFromFeatureReduction_test   ='Features_test.csv'\n",
    "\n",
    "\n",
    "    # Normalization method\n",
    "    NormalizationMethod='MinMax'\n",
    "    #NormalizationMethod='MeanStd'\n",
    "\n",
    "    # Feature Reduction Method and Settings\n",
    "    #FeatureReductionMethod='SVD'; \n",
    "    ExplainedVariance=0.99; # For method 'SVD'\n",
    "\n",
    "    FeatureReductionMethod= 'none'; \n",
    "    APpreference=-50; # Hyperparameter for method 'AffinityPropagation'\n",
    "\n",
    "\n",
    "    # Run Feature Reduction\n",
    "    FeatureReduction.RunFeatureReduction(InputToFeatureReduction_train,OutputFromFeatureReduction_train,\\\n",
    "                                         InputToFeatureReduction_test,OutputFromFeatureReduction_test,\\\n",
    "                                         NormalizationMethod,FeatureReductionMethod,ExplainedVariance,APpreference)\n",
    "    \n",
    "    data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "    data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "    \n",
    "    training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "\n",
    "    best_params = {'alpha': 0.05, 'hidden_layer_sizes': (100,100,100) }\n",
    "    model = MLPClassifier()\n",
    "    SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "    res.append(ModelPerformance.model_performance('all'))\n",
    "    print i\n",
    "\n",
    "with open(\"Model2_Results_NEW.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for m in range(len(res)):\n",
    "        writer.writerow([res[m]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(50):\n",
    "    TrainTestSplit.traintest_split(0.33)\n",
    "\n",
    "    Imputation.imputation('meanmode')\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Feature Reduction \n",
    "    # ----------------------------------\n",
    "\n",
    "    # Input and Output files from Feature Reduction for train set\n",
    "    InputToFeatureReduction_train     ='ImputedMatrix_train.csv'\n",
    "    OutputFromFeatureReduction_train  ='Features_train.csv'\n",
    "    InputToFeatureReduction_test      ='ImputedMatrix_test.csv'\n",
    "    OutputFromFeatureReduction_test   ='Features_test.csv'\n",
    "\n",
    "\n",
    "    # Normalization method\n",
    "    NormalizationMethod='MinMax'\n",
    "    #NormalizationMethod='MeanStd'\n",
    "\n",
    "    # Feature Reduction Method and Settings\n",
    "    #FeatureReductionMethod='SVD'; \n",
    "    ExplainedVariance=0.99; # For method 'SVD'\n",
    "\n",
    "    FeatureReductionMethod= 'none'; \n",
    "    APpreference=-50; # Hyperparameter for method 'AffinityPropagation'\n",
    "\n",
    "\n",
    "    # Run Feature Reduction\n",
    "    FeatureReduction.RunFeatureReduction(InputToFeatureReduction_train,OutputFromFeatureReduction_train,\\\n",
    "                                         InputToFeatureReduction_test,OutputFromFeatureReduction_test,\\\n",
    "                                         NormalizationMethod,FeatureReductionMethod,ExplainedVariance,APpreference)\n",
    "\n",
    "    data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "    data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "    \n",
    "    training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "\n",
    "    best_params = {'n_estimators': 1000}\n",
    "    model = RandomForestClassifier()\n",
    "    SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "    res.append(ModelPerformance.model_performance('all'))\n",
    "    print i\n",
    "\n",
    "\n",
    "with open(\"Model3_Results_NEW.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for m in range(len(res)):\n",
    "        writer.writerow([res[m]])\n",
    "\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(50):\n",
    "    TrainTestSplit.traintest_split(0.33)\n",
    "\n",
    "    Imputation.imputation('knn')\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Feature Reduction \n",
    "    # ----------------------------------\n",
    "\n",
    "    # Input and Output files from Feature Reduction for train set\n",
    "    InputToFeatureReduction_train     ='ImputedMatrix_train.csv'\n",
    "    OutputFromFeatureReduction_train  ='Features_train.csv'\n",
    "    InputToFeatureReduction_test      ='ImputedMatrix_test.csv'\n",
    "    OutputFromFeatureReduction_test   ='Features_test.csv'\n",
    "\n",
    "\n",
    "    # Normalization method\n",
    "    NormalizationMethod='MinMax'\n",
    "    #NormalizationMethod='MeanStd'\n",
    "\n",
    "    # Feature Reduction Method and Settings\n",
    "    #FeatureReductionMethod='SVD'; \n",
    "    ExplainedVariance=0.99; # For method 'SVD'\n",
    "\n",
    "    FeatureReductionMethod= 'none'; \n",
    "    APpreference=-50; # Hyperparameter for method 'AffinityPropagation'\n",
    "\n",
    "\n",
    "    # Run Feature Reduction\n",
    "    FeatureReduction.RunFeatureReduction(InputToFeatureReduction_train,OutputFromFeatureReduction_train,\\\n",
    "                                         InputToFeatureReduction_test,OutputFromFeatureReduction_test,\\\n",
    "                                         NormalizationMethod,FeatureReductionMethod,ExplainedVariance,APpreference)\n",
    "\n",
    "    data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "    data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "    \n",
    "    training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "\n",
    "    best_params = {'n_estimators': 1500}\n",
    "    model = RandomForestClassifier()\n",
    "    SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "    res.append(ModelPerformance.model_performance('all'))\n",
    "    print i\n",
    "\n",
    "with open(\"Model4_Results_NEW.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for m in range(len(res)):\n",
    "        writer.writerow([res[m]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Time Versus Feature Inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "TrainTestSplit.traintest_split(0.33)\n",
    "\n",
    "Imputation.imputation('knn')\n",
    "\n",
    "# ----------------------------------\n",
    "# Feature Reduction \n",
    "# ----------------------------------\n",
    "\n",
    "# Input and Output files from Feature Reduction for train set\n",
    "InputToFeatureReduction_train     ='ImputedMatrix_train.csv'\n",
    "OutputFromFeatureReduction_train  ='Features_train.csv'\n",
    "InputToFeatureReduction_test      ='ImputedMatrix_test.csv'\n",
    "OutputFromFeatureReduction_test   ='Features_test.csv'\n",
    "\n",
    "\n",
    "# Normalization method\n",
    "NormalizationMethod='MinMax'\n",
    "#NormalizationMethod='MeanStd'\n",
    "\n",
    "# Feature Reduction Method and Settings\n",
    "#FeatureReductionMethod='SVD'; \n",
    "ExplainedVariance=0.99; # For method 'SVD'\n",
    "\n",
    "FeatureReductionMethod= 'none'; \n",
    "APpreference=-50; # Hyperparameter for method 'AffinityPropagation'\n",
    "\n",
    "\n",
    "# Run Feature Reduction\n",
    "FeatureReduction.RunFeatureReduction(InputToFeatureReduction_train,OutputFromFeatureReduction_train,\\\n",
    "                                     InputToFeatureReduction_test,OutputFromFeatureReduction_test,\\\n",
    "                                     NormalizationMethod,FeatureReductionMethod,ExplainedVariance,APpreference)\n",
    "\n",
    "data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "\n",
    "training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "\n",
    "best_params = {'n_estimators': 1500}\n",
    "model = RandomForestClassifier()\n",
    "SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feature_list = list(pd.read_csv(\"Features_train.csv\"))\n",
    "feature_imp = list(pd.read_csv(\"FeatureImportances.csv\", header = None)[0])\n",
    "\n",
    "ordered_features = sorted([[feature_imp[i],feature_list[i]] for i in range(len(feature_imp))], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"Features_train.csv\")\n",
    "data_test = pd.read_csv(\"Features_test.csv\")\n",
    "names = list(data_test)\n",
    "for i in range(len(names)):\n",
    "    names[i] = names[i].replace(\" \",\"\").replace(\"#\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "data_test.columns = names\n",
    "data_train.columns = names\n",
    "\n",
    "different_features = [\n",
    "    ['CDRSB_MaxTime','CDRSB_Delta','CDRSB_Mean','CDRSB_Std'],\n",
    "    ['FAQ_MaxTime','FAQ_Delta','FAQ_Mean','FAQ_Std'],\n",
    "    ['MMSE_MaxTime','MMSE_Delta','MMSE_Mean','MMSE_Std'],\n",
    "    ['ADAS13_MaxTime','ADAS13_Delta','ADAS13_Mean','ADAS13_Std'],\n",
    "    ['EcogSPLang_MaxTime','EcogSPLang_Delta','EcogSPLang_Mean','EcogSPLang_Std','EcogSPVisspat_MaxTime','EcogSPVisspat_Delta','EcogSPVisspat_Mean','EcogSPVisspat_Std','EcogSPPlan_MaxTime','EcogSPPlan_Delta','EcogSPPlan_Mean','EcogSPPlan_Std','EcogSPOrgan_MaxTime','EcogSPOrgan_Delta','EcogSPOrgan_Mean','EcogSPOrgan_Std','EcogSPDivatt_MaxTime','EcogSPDivatt_Delta','EcogSPDivatt_Mean','EcogSPDivatt_Std','EcogSPMem_MaxTime','EcogSPMem_Delta','EcogSPMem_Mean','EcogSPMem_Std','EcogSPTotal_MaxTime','EcogSPTotal_Delta','EcogSPTotal_Mean','EcogSPTotal_Std'],\n",
    "    ['ADAS11_MaxTime','ADAS11_Delta','ADAS11_Mean','ADAS11_Std'],                 \n",
    "    ['MOCA_MaxTime','MOCA_Delta','MOCA_Mean','MOCA_Std'],\n",
    "    ['RAVLT_learning_MaxTime','RAVLT_learning_Delta','RAVLT_learning_Mean','RAVLT_learning_Std','RAVLT_forgetting_MaxTime','RAVLT_forgetting_Delta','RAVLT_forgetting_Mean','RAVLT_forgetting_Std','RAVLT_perc_forgetting_MaxTime','RAVLT_perc_forgetting_Delta','RAVLT_perc_forgetting_Mean','RAVLT_perc_forgetting_Std','RAVLT_immediate_MaxTime','RAVLT_immediate_Delta','RAVLT_immediate_Mean','RAVLT_immediate_Std'],\n",
    "    ['AGE'],\n",
    "    ['EcogPtLang_MaxTime','EcogPtLang_Delta','EcogPtLang_Mean','EcogPtLang_Std','EcogPtVisspat_MaxTime','EcogPtVisspat_Delta','EcogPtVisspat_Mean','EcogPtVisspat_Std','EcogPtPlan_MaxTime','EcogPtPlan_Delta','EcogPtPlan_Mean','EcogPtPlan_Std','EcogPtOrgan_MaxTime','EcogPtOrgan_Delta','EcogPtOrgan_Mean','EcogPtOrgan_Std','EcogPtDivatt_MaxTime','EcogPtDivatt_Delta','EcogPtDivatt_Mean','EcogPtDivatt_Std','EcogPtMem_MaxTime','EcogPtMem_Delta','EcogPtMem_Mean','EcogPtMem_Std','EcogPtTotal_MaxTime','EcogPtTotal_Delta','EcogPtTotal_Mean','EcogPtTotal_Std'],\n",
    "    [\"PTEDUCAT\"],\n",
    "    ['Years_bl'],\n",
    "    ['APOE4'],\n",
    "    ['PTMARRY_Married','PTMARRY_Nevermarried','PTMARRY_Unknown','PTMARRY_Widowed'],\n",
    "    [\"PTGENDER_Male\"], \n",
    "    [\"PTRACCAT_Asian\", \"PTRACCAT_Black\", \"PTRACCAT_Hawaiian/OtherPI\", \"PTRACCAT_Morethanone\", \"PTRACCAT_Unknown\", \"PTRACCAT_White\"],\n",
    "    [\"PTETHCAT_NotHisp/Latino\",\"PTETHCAT_Unknown\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res= []\n",
    "for i in range(len(different_features)):\n",
    "    feat_current = different_features[:i+1]\n",
    "    feat_new = []\n",
    "    for i in feat_current:\n",
    "        for j in i:\n",
    "            feat_new.append(j)\n",
    "    \n",
    "    feat_current = feat_new + ['Diagnostics']\n",
    "    current_training = data_train[feat_current].as_matrix()\n",
    "    current_test = data_test[feat_current].as_matrix()\n",
    "\n",
    "    training_set_X, test_set_X, training_set_Y, test_set_Y = current_training[:,:-1], current_test[:,:-1], current_training[:,-1], current_test[:,-1]\n",
    "\n",
    "    best_params = {'n_estimators': 1500}\n",
    "    model = RandomForestClassifier()\n",
    "    SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "    res.append(ModelPerformance.model_performance('all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.898734177215\n",
      "0.900212314225\n",
      "0.918803418803\n",
      "0.925373134328\n",
      "0.919831223629\n",
      "0.927966101695\n",
      "0.922105263158\n",
      "0.923404255319\n",
      "0.925373134328\n",
      "0.919491525424\n",
      "0.925690021231\n",
      "0.919831223629\n",
      "0.925373134328\n",
      "0.92144373673\n",
      "0.919491525424\n",
      "0.919491525424\n",
      "0.919491525424\n"
     ]
    }
   ],
   "source": [
    "for i in res:\n",
    "    print i[3]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
