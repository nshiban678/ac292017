{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to merge file AIBL_data.csv with Merged_data.csv (which is the existing ADNI file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David Castineira\\Anaconda2\\lib\\site-packages\\nbformat\\current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.worksheets[0].cells:\n",
    "                if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David Castineira\\Anaconda2\\lib\\site-packages\\pandas\\computation\\__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "#                                              Import libraries\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "#%reset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "import matplotlib as mpl\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sys import stdout\n",
    "import collections\n",
    "import csv\n",
    "import pandas as pd \n",
    "from scipy.stats import mode\n",
    "from scipy import stats\n",
    "from dateutil.parser import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MergeDiagnosis_AdjustedLabels.ipynb\n",
      "importing Jupyter notebook from Categorical_Updated.ipynb\n",
      "importing Jupyter notebook from LongitudinalDataAnalysis.ipynb\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Imputation.ipynb\n",
      "importing Jupyter notebook from FeatureReduction.ipynb\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from SupervisedLearning.ipynb\n",
      "importing Jupyter notebook from ModelPerformance.ipynb\n",
      "importing Jupyter notebook from TrainTestSplit.ipynb\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import pandas\n",
    "import csv\n",
    "import MergeDiagnosis_AdjustedLabels\n",
    "import Categorical_Updated\n",
    "import LongitudinalDataAnalysis\n",
    "import Imputation\n",
    "import FeatureReduction\n",
    "import SupervisedLearning\n",
    "import ModelPerformance\n",
    "import TrainTestSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Merge Data\n",
    "# -------------------------------\n",
    "merged_data = MergeDiagnosis_AdjustedLabels.data_preprocess(study = \"all\",imaging_to_drop = 'all', reversions = 'label0')\n",
    "from shutil import copyfile\n",
    "copyfile(\"Merged_data.csv\", \"Merged_data_original.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Data size after stacking ADNI and AIBL data sets:\n",
      "------------------------------------------------- \n",
      "(14223, 70)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Load ADNI and AIBL data sets\n",
    "# -----------------------------------------\n",
    "#\n",
    "ADNI_Data=pd.read_csv(\"Merged_data_original.csv\")\n",
    "AIBL_Data=pd.read_csv(\"AIBL_data.csv\")\n",
    "\n",
    "#Change the types of AIBL to match ADNI\n",
    "for i in list(ADNI_Data):\n",
    "    AIBL_Data[i] = AIBL_Data[i].astype(ADNI_Data[i].dtype)\n",
    "\n",
    "# Add 100000 to AIBL's RIDs to make sure they do not overlap with ADNI's RIDs\n",
    "#AIBL_Data['RID']=AIBL_Data['RID']+100000\n",
    "\n",
    "# The timestamp columns given in the AIBL file do not have the right format; we overwrite them for now (it does not matter too\n",
    "# much as they are not really used in our analysis)\n",
    "AIBL_Data['update_stamp'] = [ADNI_Data['update_stamp'].loc[0]]*len(AIBL_Data)\n",
    "\n",
    "# Stack ADNI and AIBL data sets into one big matrix\n",
    "ADNI_AIBL_data = ADNI_Data.append(AIBL_Data)\n",
    "\n",
    "# Print matrix size after stacking ADNI and AIBL data sets\n",
    "print \" \"; print \"Data size after stacking ADNI and AIBL data sets:\";print \"------------------------------------------------- \";\n",
    "print ADNI_AIBL_data.shape\n",
    "\n",
    "# Save to .csv file\n",
    "ADNI_AIBL_data.to_csv('Merged_data.csv',header=list(AIBL_Data), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that are one-hot encoded\n",
      "-------------------------------------\n",
      "['VISCODE', 'COLPROT', 'ORIGPROT', 'DX_bl', 'PTGENDER', 'PTETHCAT', 'PTRACCAT', 'PTMARRY']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Categorical to Numerical\n",
    "# -------------------------------\n",
    "date_cols = ['update_stamp','EXAMDATE','EXAMDATE_bl']\n",
    "cols_to_ignore = ['PTID']\n",
    "\n",
    "Categorical_Updated.categorical_conversion(date_cols,cols_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>RID</th>\n",
       "      <th>SITE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS11</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>...</th>\n",
       "      <th>PTRACCAT_Black</th>\n",
       "      <th>PTRACCAT_Hawaiian/Other PI</th>\n",
       "      <th>PTRACCAT_More than one</th>\n",
       "      <th>PTRACCAT_Unknown</th>\n",
       "      <th>PTRACCAT_White</th>\n",
       "      <th>PTMARRY_Married</th>\n",
       "      <th>PTMARRY_Never married</th>\n",
       "      <th>PTMARRY_Unknown</th>\n",
       "      <th>PTMARRY_Widowed</th>\n",
       "      <th>AD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>74.3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>18.67</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>81.3</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>81.3</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  RID  SITE   AGE  PTEDUCAT  APOE4  CDRSB  ADAS11  ADAS13  MMSE  \\\n",
       "0           0    2    11  74.3        16    0.0    0.0   10.67   18.67  28.0   \n",
       "1           1    3    11  81.3        18    1.0    4.5   22.00   31.00  20.0   \n",
       "2           2    3    11  81.3        18    1.0    6.0   19.00   30.00  24.0   \n",
       "\n",
       "  ...  PTRACCAT_Black  PTRACCAT_Hawaiian/Other PI  PTRACCAT_More than one  \\\n",
       "0 ...               0                           0                       0   \n",
       "1 ...               0                           0                       0   \n",
       "2 ...               0                           0                       0   \n",
       "\n",
       "   PTRACCAT_Unknown  PTRACCAT_White  PTMARRY_Married  PTMARRY_Never married  \\\n",
       "0                 0               1                1                      0   \n",
       "1                 0               1                1                      0   \n",
       "2                 0               1                1                      0   \n",
       "\n",
       "   PTMARRY_Unknown  PTMARRY_Widowed  AD  \n",
       "0                0                0   0  \n",
       "1                0                0   1  \n",
       "2                0                0   1  \n",
       "\n",
       "[3 rows x 106 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix Size:\n",
      "-----------------------\n",
      "(14223L, 106L)\n",
      " \n",
      "Identified columns of interest in input file: \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Patient RID column: [1]\n",
      "Demo columns: [  3   4  92  93  94  95  96  97  98  99 100 101 102 103 104]\n",
      "BaselineOneTime columns: [ 5 54 85 86 87]\n",
      "BaselineEvaluation columns: [30 31 32 33 35 36 37 34 38 39 41 42 43 44 45 40 46 48 49 50 51 52 47 53]\n",
      "Time columns: [ 2 56 57 58 59 60 82 83 84 55]\n",
      "CurrentEvaluation columns: [ 6  7  8  9 11 12 13 10 14 15 17 18 19 20 21 16 22 24 25 26 27 28 23 29]\n",
      "CurrentDiagnosis columns: [105]\n",
      "------\n",
      "Method 2 for Longitudinal Data Analysis\n",
      "------\n",
      " \n",
      "New Input Matrix Size:\n",
      "-----------------------\n",
      "(2598L, 116L)\n",
      " \n",
      "New Output Matrix Size:\n",
      "-----------------------\n",
      "(2598L,)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Longitudinal Data Analysis\n",
    "# -------------------------------\n",
    "\n",
    "# Input file name for Longitudinal Data Analysis\n",
    "InputToLongitudinal='CategoricalToNumerical.csv'\n",
    "\n",
    "with open(InputToLongitudinal) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    labels = next(reader)\n",
    "\n",
    "# Output file name from this script\n",
    "OutputFromLongitudinal='LongitudinalDataAnalysis.csv'\n",
    "\n",
    "# Patient RID Features\n",
    "Patient_FEATURES=['RID'];\n",
    "\n",
    "# Demographic Features\n",
    "Demo_FEATURES_type=['AGE','PTEDUCAT','PTGENDER','PTETHCAT','PTRACCAT','PTMARRY']\n",
    "Demo_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Demo_FEATURES_type:\n",
    "        Demo_FEATURES.append(i)\n",
    "    \n",
    "# Baseline OneTime Features\n",
    "BaselineOneTime_FEATURES_type = ['APOE4','Years_bl','ORIGPROT']\n",
    "BaselineOneTime_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "    elif i.rfind(\"_\") != -1 and i[:i.rfind(\"_\")] in BaselineOneTime_FEATURES_type:\n",
    "        BaselineOneTime_FEATURES.append(i)\n",
    "        \n",
    "# Time Headers\n",
    "Time_FEATURES_type=['SITE','Month','update_stamp_minus_EXAMDATE_bl','update_stamp_minus_EXAMDATE','EXAMDATE_minus_EXAMDATE_bl',\n",
    "               'COLPROT','M','Month_bl']\n",
    "Time_FEATURES = []\n",
    "for i in labels:\n",
    "    if i in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "    elif i.find(\"_\") != -1 and i[:i.find(\"_\")] in Time_FEATURES_type:\n",
    "        Time_FEATURES.append(i)\n",
    "Time_FEATURES.insert(len(Time_FEATURES), Time_FEATURES.pop(Time_FEATURES.index('Month_bl'))) # Month_bl must be last feature in this list\n",
    "\n",
    "        \n",
    "# Baseline Evaluation Features\n",
    "BaselineEvaluation_FEATURES=['CDRSB_bl','ADAS11_bl','ADAS13_bl','MMSE_bl','RAVLT_learning_bl','RAVLT_forgetting_bl',\n",
    "                             'RAVLT_perc_forgetting_bl','RAVLT_immediate_bl','FAQ_bl','MOCA_bl','EcogPtLang_bl','EcogPtVisspat_bl',\n",
    "                             'EcogPtPlan_bl','EcogPtOrgan_bl','EcogPtDivatt_bl','EcogPtMem_bl','EcogPtTotal_bl','EcogSPLang_bl',\n",
    "                             'EcogSPVisspat_bl','EcogSPPlan_bl','EcogSPOrgan_bl','EcogSPDivatt_bl','EcogSPMem_bl','EcogSPTotal_bl'];\n",
    "\n",
    "\n",
    "   \n",
    "# Current Medical Evaluation\n",
    "CurrentEvaluation_FEATURES=['CDRSB','ADAS11','ADAS13','MMSE','RAVLT_learning','RAVLT_forgetting','RAVLT_perc_forgetting','RAVLT_immediate',\n",
    "                            'FAQ','MOCA','EcogPtLang','EcogPtVisspat','EcogPtPlan','EcogPtOrgan','EcogPtDivatt','EcogPtMem','EcogPtTotal',\n",
    "                            'EcogSPLang','EcogSPVisspat','EcogSPPlan','EcogSPOrgan','EcogSPDivatt','EcogSPMem','EcogSPTotal'];\n",
    "\n",
    "\n",
    "# Current Diagnosis\n",
    "CurrentDiagnosis_FEATURES= ['AD'];\n",
    "\n",
    "# Longitudinal Method\n",
    "LongitudinalMethod=2;\n",
    "MetricList=['MaxTime','Delta','Mean','Std'];\n",
    "\n",
    "# Run Longitudinal Data Anaysis\n",
    "LongitudinalDataAnalysis.runLongitudinal(InputToLongitudinal,OutputFromLongitudinal,Patient_FEATURES,Demo_FEATURES,\\\n",
    "                                         BaselineOneTime_FEATURES,Time_FEATURES,BaselineEvaluation_FEATURES,\\\n",
    "                                         CurrentEvaluation_FEATURES,CurrentDiagnosis_FEATURES,LongitudinalMethod,MetricList)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93379790940766549, 0.79047619047619044, 0.70338983050847459, 0.74439461883408065]\n",
      "[0.93379790940766549, 0.80000000000000004, 0.69999999999999996, 0.74666666666666659]\n",
      "[0.93147502903600465, 0.80000000000000004, 0.68852459016393441, 0.74008810572687223]\n",
      "[0.93379790940766549, 0.80952380952380953, 0.69672131147540983, 0.74889867841409685]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93495934959349591, 0.80952380952380953, 0.7024793388429752, 0.75221238938053114]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93031358885017423, 0.79047619047619044, 0.68595041322314054, 0.73451327433628311]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93031358885017423, 0.80952380952380953, 0.68000000000000005, 0.73913043478260887]\n",
      "[0.92915214866434381, 0.79047619047619044, 0.68032786885245899, 0.7312775330396476]\n",
      "[0.93263646922183507, 0.79047619047619044, 0.69747899159663862, 0.74107142857142849]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93379790940766549, 0.80000000000000004, 0.69999999999999996, 0.74666666666666659]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.92915214866434381, 0.79047619047619044, 0.68032786885245899, 0.7312775330396476]\n",
      "[0.93031358885017423, 0.79047619047619044, 0.68595041322314054, 0.73451327433628311]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.92799070847851339, 0.80952380952380953, 0.6692913385826772, 0.73275862068965525]\n",
      "[0.93031358885017423, 0.80952380952380953, 0.68000000000000005, 0.73913043478260887]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93031358885017423, 0.80952380952380953, 0.68000000000000005, 0.73913043478260887]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93147502903600465, 0.80000000000000004, 0.68852459016393441, 0.74008810572687223]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93379790940766549, 0.80000000000000004, 0.69999999999999996, 0.74666666666666659]\n",
      "[0.92915214866434381, 0.79047619047619044, 0.68032786885245899, 0.7312775330396476]\n",
      "[0.93379790940766549, 0.80000000000000004, 0.69999999999999996, 0.74666666666666659]\n",
      "[0.92915214866434381, 0.79047619047619044, 0.68032786885245899, 0.7312775330396476]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93147502903600465, 0.80000000000000004, 0.68852459016393441, 0.74008810572687223]\n",
      "[0.93031358885017423, 0.80952380952380953, 0.68000000000000005, 0.73913043478260887]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93147502903600465, 0.80000000000000004, 0.68852459016393441, 0.74008810572687223]\n",
      "[0.93379790940766549, 0.80000000000000004, 0.69999999999999996, 0.74666666666666659]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93379790940766549, 0.80952380952380953, 0.69672131147540983, 0.74889867841409685]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93379790940766549, 0.79047619047619044, 0.70338983050847459, 0.74439461883408065]\n",
      "[0.93147502903600465, 0.80000000000000004, 0.68852459016393441, 0.74008810572687223]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93031358885017423, 0.80952380952380953, 0.68000000000000005, 0.73913043478260887]\n",
      "[0.93612078977932633, 0.80952380952380953, 0.70833333333333337, 0.75555555555555565]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93147502903600465, 0.81904761904761902, 0.68253968253968256, 0.74458874458874458]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93379790940766549, 0.80952380952380953, 0.69672131147540983, 0.74889867841409685]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.92682926829268297, 0.80000000000000004, 0.66666666666666663, 0.72727272727272718]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93495934959349591, 0.80000000000000004, 0.70588235294117652, 0.75000000000000011]\n",
      "[0.93379790940766549, 0.80000000000000004, 0.69999999999999996, 0.74666666666666659]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93031358885017423, 0.80952380952380953, 0.68000000000000005, 0.73913043478260887]\n",
      "[0.93379790940766549, 0.79047619047619044, 0.70338983050847459, 0.74439461883408065]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93147502903600465, 0.80000000000000004, 0.68852459016393441, 0.74008810572687223]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93031358885017423, 0.80952380952380953, 0.68000000000000005, 0.73913043478260887]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93379790940766549, 0.80952380952380953, 0.69672131147540983, 0.74889867841409685]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93147502903600465, 0.80952380952380953, 0.68548387096774188, 0.74235807860262004]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n",
      "[0.93031358885017423, 0.79047619047619044, 0.68595041322314054, 0.73451327433628311]\n",
      "[0.93147502903600465, 0.80000000000000004, 0.68852459016393441, 0.74008810572687223]\n",
      "[0.93147502903600465, 0.80000000000000004, 0.68852459016393441, 0.74008810572687223]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93263646922183507, 0.81904761904761902, 0.68799999999999994, 0.74782608695652164]\n",
      "[0.93379790940766549, 0.80952380952380953, 0.69672131147540983, 0.74889867841409685]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93379790940766549, 0.80952380952380953, 0.69672131147540983, 0.74889867841409685]\n",
      "[0.93263646922183507, 0.80952380952380953, 0.69105691056910568, 0.74561403508771928]\n",
      "[0.93031358885017423, 0.80000000000000004, 0.68292682926829273, 0.73684210526315796]\n",
      "[0.93147502903600465, 0.79047619047619044, 0.69166666666666665, 0.73777777777777775]\n",
      "[0.93263646922183507, 0.80000000000000004, 0.69421487603305787, 0.74336283185840701]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # ------------------\n",
    "    # Train Test Split\n",
    "    # ------------------\n",
    "    TrainTestSplit.traintest_split(0.33)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # Keep only a few columns that are common to both ADNI and AIBL\n",
    "    # --------------------------------------------------------------\n",
    "    f=pd.read_csv(\"LongitudinalDataAnalysis_test.csv\")\n",
    "    keep_col = ['# AGE','PTGENDER_Male','APOE4','ORIGPROT_ADNI2','ORIGPROT_ADNIGO','ORIGPROT_AIBL','MMSE_MaxTime','MMSE_Delta','MMSE_Mean','MMSE_Std','Diagnostics']\n",
    "    new_f = f[keep_col]\n",
    "    new_f.to_csv(\"LongitudinalDataAnalysis_test.csv\", index=False)\n",
    "\n",
    "    f=pd.read_csv(\"LongitudinalDataAnalysis_train.csv\")\n",
    "    keep_col = ['# AGE','PTGENDER_Male','APOE4','ORIGPROT_ADNI2','ORIGPROT_ADNIGO','ORIGPROT_AIBL','MMSE_MaxTime','MMSE_Delta','MMSE_Mean','MMSE_Std','Diagnostics']\n",
    "    new_f = f[keep_col]\n",
    "    new_f.to_csv(\"LongitudinalDataAnalysis_train.csv\", index=False)\n",
    "\n",
    "    # ------------------\n",
    "    # Imputation\n",
    "    # ------------------\n",
    "    #Imputation.imputation('knn')\n",
    "    Imputation.imputation('meanmode')\n",
    "    #Imputation.imputation('nuclearnorm')\n",
    "    #Imputation.imputation('softimpute')\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Feature Reduction \n",
    "    # ----------------------------------\n",
    "\n",
    "    # Input and Output files from Feature Reduction for train set\n",
    "    InputToFeatureReduction_train     ='ImputedMatrix_train.csv'\n",
    "    OutputFromFeatureReduction_train  ='Features_train.csv'\n",
    "    InputToFeatureReduction_test      ='ImputedMatrix_test.csv'\n",
    "    OutputFromFeatureReduction_test   ='Features_test.csv'\n",
    "\n",
    "\n",
    "    # Normalization method\n",
    "    NormalizationMethod='MinMax'\n",
    "    #NormalizationMethod='MeanStd'\n",
    "\n",
    "    # Feature Reduction Method and Settings\n",
    "    #FeatureReductionMethod='SVD'; \n",
    "    ExplainedVariance=0.99; # For method 'SVD'\n",
    "\n",
    "    FeatureReductionMethod='none'; \n",
    "    APpreference=-50; # Hyperparameter for method 'AffinityPropagation'\n",
    "\n",
    "\n",
    "    # Run Feature Reduction\n",
    "    FeatureReduction.RunFeatureReduction(InputToFeatureReduction_train,OutputFromFeatureReduction_train,\\\n",
    "                                         InputToFeatureReduction_test,OutputFromFeatureReduction_test,\\\n",
    "                                         NormalizationMethod,FeatureReductionMethod,ExplainedVariance,APpreference)\n",
    "\n",
    "    # ------------------\n",
    "    # Supervised Learning- Model 3\n",
    "    # ------------------\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import numpy as np\n",
    "    data_train = np.loadtxt('Features_train.csv', delimiter=\",\", skiprows = 1)\n",
    "    data_test = np.loadtxt('Features_test.csv', delimiter=\",\", skiprows = 1)\n",
    "    training_set_X, test_set_X, training_set_Y, test_set_Y = data_train[:,:-1], data_test[:,:-1], data_train[:,-1], data_test[:,-1]\n",
    "    best_params = {'n_estimators': 1000}\n",
    "    model = RandomForestClassifier()\n",
    "    SupervisedLearning.test_model(training_set_X, test_set_X, training_set_Y, test_set_Y, model, best_params)\n",
    "    res.append(ModelPerformance.model_performance('all'))\n",
    "\n",
    "    #------------------\n",
    "    # Model Evaluation\n",
    "    # ------------------\n",
    "    #print ModelPerformance.model_performance('confusion_matrix')\n",
    "    print ModelPerformance.model_performance('all')\n",
    "    \n",
    "    res.append(ModelPerformance.model_performance('all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93184669,  0.80333333,  0.68933623,  0.74194203])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print res\n",
    "res2=np.asarray(res)\n",
    "#print res2\n",
    "np.mean(res2, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
