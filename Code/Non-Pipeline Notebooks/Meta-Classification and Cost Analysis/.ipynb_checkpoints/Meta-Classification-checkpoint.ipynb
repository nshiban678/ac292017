{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "d_meta = pd.read_csv('MetaClassifier_Dataset_NEWSize3.csv')\n",
    "labels = list(d_meta)\n",
    "data = d_meta.as_matrix()[:,:] \n",
    "\n",
    "Y = data[:, -1]\n",
    "X = data[:,0:-1]\n",
    "#\n",
    "#training_set_X = X\n",
    "#training_set_Y = Y\n",
    "training_set_X, test_set_X, training_set_Y, test_set_Y, = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This block calculates the costs for the different models as well as the conditional cost \n",
    "\n",
    "feature_costs = {'1':20, '2':20,'3':0,'4':30, '5':45, '6':45,'7':10,'8':45,'9':10,'10':12,'11':10,'12':10}\n",
    "model_info = {}\n",
    "with open(\"Models_MetaClassifier_NEW.csv\", \"r\") as ins:\n",
    "    for line in ins:\n",
    "        info = line.rstrip('\\n').rstrip('\\r').split(\",\")\n",
    "        model_info[info[0]] = info[1:]\n",
    "\n",
    "global model_cost \n",
    "model_cost = []\n",
    "for i in labels[:-1]:\n",
    "    model_num = i[i.index('l')+1:]\n",
    "    model_string = 'Model '+str(model_num)\n",
    "    cost = 0\n",
    "    for feature in model_info[model_string]:\n",
    "        if feature != \"\":\n",
    "            cost += feature_costs[feature]\n",
    "    model_cost.append(cost)\n",
    "    \n",
    "global model_condition_cost \n",
    "model_condition_cost = {}\n",
    "for i in labels[:-1]:\n",
    "    model_num1 = i[i.index('l')+1:]\n",
    "    model_string = 'Model '+str(model_num1)\n",
    "    model_condition_cost[int(model_num1)] = {}\n",
    "    for j in labels[:-1]:\n",
    "        model_num2 = j[j.index('l')+1:]\n",
    "        model_string2 = 'Model '+str(model_num2)\n",
    "        \n",
    "        features1 = model_info[model_string]\n",
    "        features2 = model_info[model_string2]\n",
    "        additional_cost = 0\n",
    "        for f in features2:\n",
    "            if f not in features1:\n",
    "                additional_cost += feature_costs[f]\n",
    "                \n",
    "        model_condition_cost[int(model_num1)][int(model_num2)] = additional_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionNode(object):\n",
    "    \"\"\"Makes Decision Node Class\"\"\"\n",
    "    def __init__(self,feature=None,left=None,right=None,classes = None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = feature\n",
    "        self.classes = classes\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.left == None and self.right == None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.feature == None:\n",
    "            return str(Counter(self.classes))\n",
    "        return \"Decision node for feature \" + str(self.feature)\n",
    "\n",
    "class DecisionTree(object):\n",
    "    \"\"\"Decision Tree Class\"\"\"\n",
    "    def __init__(self):\n",
    "        self.root_node = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predicted_classes = []\n",
    "\n",
    "        for sample in X:\n",
    "            c = None\n",
    "            current_node = self.root_node\n",
    "            while c is None:\n",
    "                if current_node.is_leaf():\n",
    "                    try:\n",
    "                        c = int(current_node.classes)\n",
    "                    except:\n",
    "                        ones = sum(current_node.classes)\n",
    "                        zeros = len(current_node.classes)-ones\n",
    "                        if ones > zeros:\n",
    "                            c = 1\n",
    "                        else:\n",
    "                            c = 0\n",
    "                    \n",
    "                else:\n",
    "                    key_value = sample[current_node.feature]\n",
    "                    if key_value == 0:\n",
    "                        current_node = current_node.left\n",
    "                    elif key_value == 1:\n",
    "                        current_node = current_node.right\n",
    "            predicted_classes.append(c)\n",
    "        return predicted_classes\n",
    "    \n",
    "    def fit(self,samples,outcome_variable, depth_max):\n",
    "        \"\"\"Takes in training data and builds a decision tree\n",
    "        samples = X values as list of list\n",
    "        outcome_varaible = Y value\n",
    "        \"\"\"\n",
    "        training_samples = [(s, t) for s, t in zip(samples, outcome_variable)]\n",
    "        predicting_features = list(range(len(samples[0])))\n",
    "        random.shuffle(predicting_features)\n",
    "        self.root_node = self.build_decision_tree(training_samples,predicting_features,0, depth_max)\n",
    "        \n",
    "    def build_decision_tree(self,samples,features, depth, depth_max):\n",
    "        classes = [sample[1] for sample in samples]\n",
    "        \n",
    "        #if we only have one class, return just a decision node \n",
    "        if len(set(classes)) == 1:\n",
    "            root_node = DecisionNode(feature=None, left=None, right=None, classes = classes)\n",
    "        \n",
    "        #if the depth is greater than the max_depth, return a decision node\n",
    "        elif depth >= depth_max:\n",
    "            return  DecisionNode(feature=None, left=None, right=None, classes = [sample[1] for sample in samples])\n",
    "        \n",
    "        #if there are no more remaining features to try, return decision node\n",
    "        elif features == []:\n",
    "            return  DecisionNode(feature=None, left=None, right=None, classes = [sample[1] for sample in samples])\n",
    "        \n",
    "        #otherwise, recurse\n",
    "        else:\n",
    "            best_feature = self.select_best_feature(samples,features,classes) #determine best feature\n",
    "            print \"BEST FEATURE THIS ITERATION IS \", best_feature\n",
    "            \n",
    "            #if the feature has a value of info_gain/estimated_cost that is <= 0, stop the algorithm\n",
    "            if best_feature == 'stop':\n",
    "                root_node = DecisionNode(feature=None, left=None, right=None, classes = [sample[1] for sample in samples])\n",
    "                return root_node\n",
    "            \n",
    "            best_feature_values = [s[0][best_feature] for s in samples]\n",
    "            \n",
    "            #if we are at pure leaf\n",
    "            if len(best_feature_values) == 1:\n",
    "                root_node = DecisionNode(feature = best_feature, classes = best_feature_values[0])\n",
    "           \n",
    "            else:\n",
    "                #do left hand side\n",
    "                left_samples = [s for s in samples if s[0][best_feature] == 0]\n",
    "                left_node = self.build_decision_tree(left_samples,features, depth + 1,depth_max)\n",
    "                \n",
    "                #do right hand side\n",
    "                right_samples = [s for s in samples if s[0][best_feature] == 1]\n",
    "                right_node = self.build_decision_tree(right_samples,features, depth + 1,depth_max)\n",
    "\n",
    "                root_node = DecisionNode(feature = best_feature, classes = best_feature_values, left = left_node, right= right_node)\n",
    "\n",
    "        return root_node\n",
    "    \n",
    "    \n",
    "    def print_tree(self,labels):\n",
    "        curr_node = self.root_node\n",
    "        print self.__str__(curr_node,0)\n",
    "\n",
    "    def __str__(self, node, depth=0):\n",
    "        ret = \"\"\n",
    "        # Print right branch\n",
    "        if node.right != None:\n",
    "            ret += self.__str__(node.right,depth + 1)\n",
    "        # Print own value\n",
    "        if node.feature != None:\n",
    "            ret += \"\\n\" + (\"    \"*depth) + str(node.feature)\n",
    "        else:\n",
    "            ret += \"\\n\" + (\"    \"*depth) + \"Class: \" + str(node)\n",
    "        # Print left branch\n",
    "        if node.left != None:\n",
    "            ret += self.__str__(node.left,depth + 1)\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def select_best_feature(self, samples, features, classes):\n",
    "        \"\"\"\n",
    "        Find score for all remaining features, choose the one that maximizes\n",
    "        the score function and delete this feature from consideration\n",
    "        \"\"\"\n",
    "        gain_factors = [(self.score_function(samples, feat, classes, features), feat)\n",
    "                        for feat in features]\n",
    "        gain_factors.sort()\n",
    "        #print \"GAIN FACTORS\", gain_factors\n",
    "        \n",
    "        best_feature = gain_factors[-1][1]\n",
    "        \n",
    "        if gain_factors[-1][0] <= 0:\n",
    "            return \"stop\"\n",
    "        #features.pop(features.index(best_feature))\n",
    "        return best_feature\n",
    "\n",
    "\n",
    "    def information_gain(self, samples, feature, classes):\n",
    "        \"\"\"\n",
    "        Information gain is the measure of the difference in entropy from before\n",
    "        to after the samples are split on the given feature values. In other\n",
    "        words, how much uncertainty in the samples was reduced after splitting\n",
    "        them on the given feature.\n",
    "        \"\"\"\n",
    "        N = len(samples)\n",
    "        samples_partition = defaultdict(list)\n",
    "        for s in samples:\n",
    "            samples_partition[s[0][feature]].append(s)\n",
    "\n",
    "        feature_entropy = 0.0\n",
    "        subclasses = []\n",
    "        #print \"NUMB PART\",len(samples_partition)\n",
    "        for partition in samples_partition.values():\n",
    "            \n",
    "            sub_classes = [s[1] for s in partition]\n",
    "            sub_entropy = self.entropy(sub_classes)\n",
    "            subclasses.append(Counter(sub_classes))\n",
    "            #print \"SUBENTROPY\",sub_entropy\n",
    "            feature_entropy += (len(partition) / float(N)) * sub_entropy \n",
    "            #print feature_entropy\n",
    "        \n",
    "        \n",
    "        p = self.entropy(classes)\n",
    "\n",
    "        #if round(p,16) < round(feature_entropy,16):\n",
    "        #    print \"PARENT HAS SMALLER ENTROPY THAN CHILD\", p, feature_entropy, Counter(classes), subclasses\n",
    "            \n",
    "        return p, feature_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(dataset):\n",
    "        \"\"\"Measure of the amount of uncertainty in the given dataset.\"\"\"\n",
    "\n",
    "        N = len(dataset)\n",
    "        counter = Counter(dataset)\n",
    "        return sum([-1.0*(counter[k] / float(N))*math.log(counter[k] / float(N),2) for k in counter])\n",
    "    #\n",
    "    def cost(self, feature):\n",
    "        global model_cost\n",
    "        return model_cost[feature]\n",
    "    \n",
    "    def cost_conditional(self, feature, feature2):\n",
    "        global model_condition_cost\n",
    "        return model_condition_cost[feature][feature2]\n",
    "    \n",
    "    def P_L(self, samples, feature, classes):\n",
    "        parent_entrop, child_entrop = self.information_gain(samples, feature, classes)\n",
    "        pl = 1- ((child_entrop)/float(parent_entrop))\n",
    "        return pl\n",
    "    \n",
    "    \n",
    "    def P_L_conditional(self, samples, feature, classes, features_left):\n",
    "        samples_partition = defaultdict(list)\n",
    "        for s in samples:\n",
    "            samples_partition[s[0][feature]].append(s)\n",
    "         \n",
    "        sums = 0\n",
    "        #print \"ORIG\"\n",
    "        for i in samples_partition: #either 0 or 1\n",
    "            #get subclasses\n",
    "            sub_classes = [s[1] for s in samples_partition[i]]\n",
    "            \n",
    "            #for each feature left\n",
    "            for f in features_left:\n",
    "                \n",
    "                if len(list(set(sub_classes))) > 1:\n",
    "                    pl = self.P_L(samples_partition[i], f, sub_classes)\n",
    "                \n",
    "                #if its a pure class, pl = 1\n",
    "                else:\n",
    "                    pl = 1\n",
    "                sums += pl*self.cost_conditional(feature,f)\n",
    "        return sums\n",
    "\n",
    "    \n",
    "    \n",
    "    def Estimated_Cost(self, samples, feature, classes, features_left):\n",
    "        PL = self.P_L(samples, feature, classes)\n",
    "        #if PL > 1 or PL < 0:\n",
    "        #    print \"PL\", PL\n",
    "        if PL == 1:\n",
    "            return PL*self.cost(feature)\n",
    "        \n",
    "        first_part = PL*self.cost(feature)\n",
    "        second_part = (1-PL)*self.P_L_conditional(samples, feature, classes, features_left) \n",
    "        \n",
    "        return first_part + second_part\n",
    "    \n",
    "    \n",
    "    \n",
    "    def score_function(self,samples, feature, classes, features_left):\n",
    "        #print \"CURRENT FEATURE IS\", feature\n",
    "        info_gain1, info_gain2 = self.information_gain(samples, feature, classes)\n",
    "        #print 'INFORMATION GAIN PARENT',info_gain1,\"CHILD\",info_gain2\n",
    "        estimated_cost = self.Estimated_Cost(samples, feature, classes, features_left)\n",
    "        if estimated_cost == 0:\n",
    "            return 0\n",
    "        if estimated_cost < 0:\n",
    "            print \"estimated cost < 0\"\n",
    "        #print \"GAIN AND COST\",info_gain1-info_gain2, estimated_cost\n",
    "        return (info_gain1-info_gain2)/float(estimated_cost)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Function to print the Decision Tree Classifier from sklearn to compare results \"\"\"\n",
    "def get_code(tree, feature_names):\n",
    "        left      = tree.tree_.children_left\n",
    "        right     = tree.tree_.children_right\n",
    "        threshold = tree.tree_.threshold\n",
    "        features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "        value = tree.tree_.value\n",
    "\n",
    "        def recurse(left, right, threshold, features, node):\n",
    "                if (threshold[node] != -2):\n",
    "                        print \"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\"\n",
    "                        if left[node] != -1:\n",
    "                                recurse (left, right, threshold, features,left[node])\n",
    "                        print \"} else {\"\n",
    "                        if right[node] != -1:\n",
    "                                recurse (left, right, threshold, features,right[node])\n",
    "                        print \"}\"\n",
    "                else:\n",
    "                        print \"return \" + str(value[node])\n",
    "\n",
    "        recurse(left, right, threshold, features, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Small Test Examples to Check Work\n",
    "\n",
    "samples = np.array([[0,0,1,0,1],[1,1,1,1,0],[1,0,0,1,0],[1,0,0,1,0],[1,0,0,1,0]])\n",
    "targets = np.array([1,0,1,0,1])\n",
    "\n",
    "#samples = [[0,0,0,0,1],[1,1,1,1,0],[0,0,0,1,0],[1,0,0,1,0]]\n",
    "#targets = [1,0,0,1]\n",
    "\n",
    "#samples = [[0,0,1,0,1],[1,1,1,1,0],[0,0,0,1,0],[1,0,0,1,0]]\n",
    "#targets = [1,0,1,1]\n",
    "\n",
    "#samples = [[0,0,1,0,1],[1,1,1,1,0],[0,0,0,1,1],[1,0,0,1,0],[0,1,1,1,0],[1,1,0,0,0]]\n",
    "#targets = [1,0,1,1,0,1]\n",
    "\n",
    "#samples = [[1,1,1,0,1],[1,1,0,1,0],[0,0,0,1,1],[0,0,0,1,0],[0,1,1,1,0],[1,1,0,0,0]]\n",
    "#targets = [1,0,1,0,0,1]\n",
    "\n",
    "\n",
    "d = DecisionTree()\n",
    "d.fit(samples,targets,[],3)\n",
    "d.print_tree(range(4))\n",
    "\n",
    "print \"\\nSklearn tree\"\n",
    "dtree = tree.DecisionTreeClassifier(criterion = 'entropy')\n",
    "dtree = dtree.fit(samples,targets)\n",
    "get_code(dtree,['0','1','2','3','4','5'])\n",
    "\n",
    "print d.predict([[0,0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST FEATURE THIS ITERATION IS  224\n",
      "\n",
      "    Class: Counter({1: 130, 0: 17})\n",
      "224\n",
      "    Class: Counter({0: 226, 1: 11})\n",
      "0.910526315789\n",
      "0.898734177215\n"
     ]
    }
   ],
   "source": [
    "#How to Fit and Test Model\n",
    "\n",
    "d = DecisionTree()\n",
    "d.fit(training_set_X,training_set_Y,1)\n",
    "d.print_tree(labels)\n",
    "\n",
    "y_pred = d.predict(test_set_X)\n",
    "\n",
    "count = []\n",
    "recall = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == test_set_Y[i]:\n",
    "        count.append(1)\n",
    "    else:\n",
    "        count.append(0)\n",
    "        \n",
    "    if test_set_Y[i] == 1:\n",
    "        if y_pred[i] == 1:\n",
    "            recall.append(1)\n",
    "        else:\n",
    "            recall.append(0)\n",
    "            \n",
    "print sum(count)/float(len(count))\n",
    "print sum(recall)/float(len(recall))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
